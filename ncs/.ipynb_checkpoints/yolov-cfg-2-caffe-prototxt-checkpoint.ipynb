{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data/ssd-caffe/py2_caffe/python/', '/data/ssd-caffe/py2_caffe/python/', '/data/ssd-caffe/py2_caffe/python/', '/data/ssd-caffe/py2_caffe/python/', '/data/ssd-caffe/py2_caffe/python/', '', '/data/github_repos/darknet/ncs', '/opt/movidius/caffe/python', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-x86_64-linux-gnu', '/usr/lib/python2.7/lib-tk', '/usr/lib/python2.7/lib-old', '/usr/lib/python2.7/lib-dynload', '/home/yyp/.local/lib/python2.7/site-packages', '/usr/local/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages', '/home/yyp/.local/lib/python2.7/site-packages/IPython/extensions', '/home/yyp/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "sys.path.insert(0,'/data/ssd-caffe/py2_caffe/python/')\n",
    "import caffe  \n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from ConfigParser import ConfigParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniqDict(OrderedDict):\n",
    "    _unique = 0\n",
    "    def __setitem__(self, key, val):\n",
    "        if isinstance(val, OrderedDict):\n",
    "            self._unique += 1\n",
    "            key += \"_\"+str(self._unique)\n",
    "        OrderedDict.__setitem__(self, key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cfg(cfgfile):\n",
    "    parser = ConfigParser(dict_type=UniqDict)\n",
    "    parser.read(cfgfile)\n",
    "    blocks = parser.sections()\n",
    "    return blocks, parser\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfg2prototxt(cfgfile):\n",
    "    blocks, parser = parse_cfg(cfgfile)\n",
    "    print(blocks)\n",
    "    layers = []  \n",
    "    props = OrderedDict()  \n",
    "    bottom = 'data'  \n",
    "    layer_id = 1  \n",
    "    topnames = dict()  \n",
    "    for block in blocks:\n",
    "        items = dict(parser.items(block))\n",
    "        items['type'] = block.split('_')[0]\n",
    "        print(items)\n",
    "        if items['type'] == 'net':  \n",
    "            props['name'] = 'Darkent2Caffe'  \n",
    "            props['input'] = 'data'  \n",
    "            props['input_dim'] = ['1']  \n",
    "            props['input_dim'].append(items['channels'])  \n",
    "            props['input_dim'].append(items['height'])  \n",
    "            props['input_dim'].append(items['width'])  \n",
    "            continue  \n",
    "        elif items['type'] == 'convolutional':  \n",
    "            conv_layer = OrderedDict()  \n",
    "            conv_layer['bottom'] = bottom  \n",
    "            if items.has_key('name'):  \n",
    "                conv_layer['top'] = items['name']  \n",
    "                conv_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                conv_layer['top'] = 'layer%d-conv' % layer_id  \n",
    "                conv_layer['name'] = 'layer%d-conv' % layer_id  \n",
    "            conv_layer['type'] = 'Convolution'  \n",
    "            convolution_param = OrderedDict()  \n",
    "            convolution_param['num_output'] = items['filters']  \n",
    "            convolution_param['kernel_size'] = items['size']  \n",
    "            if items['pad'] == '1':  \n",
    "                convolution_param['pad'] = str(int(convolution_param['kernel_size'])/2)  \n",
    "            convolution_param['stride'] = items['stride']\n",
    "            if items.has_key('batch_normalize'):\n",
    "                if items['batch_normalize'] == '1':  \n",
    "                    convolution_param['bias_term'] = 'false' \n",
    "                else:  \n",
    "                    convolution_param['bias_term'] = 'true'  \n",
    "            conv_layer['convolution_param'] = convolution_param  \n",
    "            layers.append(conv_layer)  \n",
    "            bottom = conv_layer['top']  \n",
    "            if items.has_key('batch_normalize'):\n",
    "                if items['batch_normalize'] == '1': \n",
    "                    bn_layer = OrderedDict()  \n",
    "                    bn_layer['bottom'] = bottom  \n",
    "                    bn_layer['top'] = bottom  \n",
    "                    if items.has_key('name'):  \n",
    "                        bn_layer['name'] = '%s-bn' % items['name']  \n",
    "                    else:  \n",
    "                        bn_layer['name'] = 'layer%d-bn' % layer_id  \n",
    "                    bn_layer['type'] = 'BatchNorm'  \n",
    "                    batch_norm_param = OrderedDict()  \n",
    "                    batch_norm_param['use_global_stats'] = 'true'  \n",
    "                    bn_layer['batch_norm_param'] = batch_norm_param  \n",
    "                    layers.append(bn_layer)  \n",
    "\n",
    "                    scale_layer = OrderedDict()  \n",
    "                    scale_layer['bottom'] = bottom  \n",
    "                    scale_layer['top'] = bottom  \n",
    "                    if items.has_key('name'):  \n",
    "                        scale_layer['name'] = '%s-scale' % items['name']  \n",
    "                    else:  \n",
    "                        scale_layer['name'] = 'layer%d-scale' % layer_id  \n",
    "                    scale_layer['type'] = 'Scale'  \n",
    "                    scale_param = OrderedDict()  \n",
    "                    scale_param['bias_term'] = 'true'  \n",
    "                    scale_layer['scale_param'] = scale_param  \n",
    "                    layers.append(scale_layer)  \n",
    "\n",
    "            if items['activation'] != 'linear':  \n",
    "                relu_layer = OrderedDict()  \n",
    "                relu_layer['bottom'] = bottom  \n",
    "                relu_layer['top'] = bottom  \n",
    "                if items.has_key('name'):  \n",
    "                    relu_layer['name'] = '%s-act' % items['name']  \n",
    "                else:  \n",
    "                    relu_layer['name'] = 'layer%d-act' % layer_id  \n",
    "                relu_layer['type'] = 'ReLU'  \n",
    "                if items['activation'] == 'leaky':  \n",
    "                    relu_param = OrderedDict()  \n",
    "                    relu_param['negative_slope'] = '0.1'  \n",
    "                    relu_layer['relu_param'] = relu_param  \n",
    "                layers.append(relu_layer)  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id+1  \n",
    "        elif items['type'] == 'maxpool':  \n",
    "            max_layer = OrderedDict()  \n",
    "            max_layer['bottom'] = bottom  \n",
    "            if items.has_key('name'):  \n",
    "                max_layer['top'] = items['name']  \n",
    "                max_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                max_layer['top'] = 'layer%d-maxpool' % layer_id  \n",
    "                max_layer['name'] = 'layer%d-maxpool' % layer_id  \n",
    "            max_layer['type'] = 'Pooling'  \n",
    "            pooling_param = OrderedDict()  \n",
    "            pooling_param['kernel_size'] = items['size']  \n",
    "            pooling_param['stride'] = items['stride']  \n",
    "            pooling_param['pool'] = 'MAX'  \n",
    "            if items.has_key('pad') and int(items['pad']) == 1:  \n",
    "                pooling_param['pad'] = str((int(items['size'])-1)/2)  \n",
    "            max_layer['pooling_param'] = pooling_param  \n",
    "            layers.append(max_layer)  \n",
    "            bottom = max_layer['top']  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id+1  \n",
    "        elif items['type'] == 'avgpool':  \n",
    "            avg_layer = OrderedDict()  \n",
    "            avg_layer['bottom'] = bottom  \n",
    "            if block.has_key('name'):  \n",
    "                avg_layer['top'] = items['name']  \n",
    "                avg_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                avg_layer['top'] = 'layer%d-avgpool' % layer_id  \n",
    "                avg_layer['name'] = 'layer%d-avgpool' % layer_id  \n",
    "            avg_layer['type'] = 'Pooling'  \n",
    "            pooling_param = OrderedDict()  \n",
    "            pooling_param['kernel_size'] = 7  \n",
    "            pooling_param['stride'] = 1  \n",
    "            pooling_param['pool'] = 'AVE'  \n",
    "            avg_layer['pooling_param'] = pooling_param  \n",
    "            layers.append(avg_layer)  \n",
    "            bottom = avg_layer['top']  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id+1  \n",
    "        elif items['type'] == 'yolo': \n",
    "            region_layer = OrderedDict()  \n",
    "            region_layer['bottom'] = bottom  \n",
    "            if items.has_key('name'):  \n",
    "                region_layer['top'] = items['name']  \n",
    "                region_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                region_layer['top'] = 'layer%d-yolo' % layer_id  \n",
    "                region_layer['name'] = 'layer%d-yolo' % layer_id  \n",
    "            region_layer['type'] = 'Yolo'  \n",
    "            region_param = OrderedDict()  \n",
    "            region_param['anchors'] = items['anchors'].strip()  \n",
    "            region_param['classes'] = items['classes']  \n",
    "            region_param['num'] = items['num']  \n",
    "            region_layer['yolo_param'] = region_param  \n",
    "            layers.append(region_layer)  \n",
    "            bottom = region_layer['top']  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id + 1  \n",
    "\n",
    "        elif items['type'] == 'route':\n",
    "            route_layer = OrderedDict()  \n",
    "            layer_name = str(items['layers']).split(',')  \n",
    "            print(layer_name[0])  \n",
    "            bottom_layer_size = len(str(items['layers']).split(','))  \n",
    "        #print(bottom_layer_size)  \n",
    "            if(1 == bottom_layer_size):  \n",
    "                prev_layer_id = layer_id + int(items['layers'])  \n",
    "                bottom = topnames[prev_layer_id]  \n",
    "                #topnames[layer_id] = bottom  \n",
    "            route_layer['bottom'] = bottom  \n",
    "            if(2 == bottom_layer_size):  \n",
    "                prev_layer_id1 = layer_id + int(layer_name[0])  \n",
    "                #print(prev_layer_id1)  \n",
    "                prev_layer_id2 = int(layer_name[1]) + 1  \n",
    "                print(topnames)  \n",
    "                bottom1 = topnames[prev_layer_id1]  \n",
    "                bottom2 = topnames[prev_layer_id2]  \n",
    "                route_layer['bottom'] = [bottom1, bottom2]  \n",
    "            if items.has_key('name'):  \n",
    "                route_layer['top'] = items['name']  \n",
    "                route_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                route_layer['top'] = 'layer%d-route' % layer_id  \n",
    "                route_layer['name'] = 'layer%d-route' % layer_id  \n",
    "            route_layer['type'] = 'Concat'\n",
    "            print(route_layer)  \n",
    "            layers.append(route_layer)  \n",
    "            bottom = route_layer['top']  \n",
    "            print(layer_id)  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id + 1  \n",
    "\n",
    "        elif items['type'] == 'upsample':\n",
    "            upsample_layer = OrderedDict()  \n",
    "            print(items['stride'])  \n",
    "            upsample_layer['bottom'] = bottom  \n",
    "            if items.has_key('name'):  \n",
    "                upsample_layer['top'] = items['name']  \n",
    "                upsample_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                upsample_layer['top'] = 'layer%d-upsample' % layer_id  \n",
    "                upsample_layer['name'] = 'layer%d-upsample' % layer_id  \n",
    "            upsample_layer['type'] = 'Upsample'  \n",
    "            upsample_param = OrderedDict()  \n",
    "            upsample_param['scale'] = items['stride']  \n",
    "            upsample_layer['upsample_param'] = upsample_param  \n",
    "            print(upsample_layer)  \n",
    "            layers.append(upsample_layer)  \n",
    "            bottom = upsample_layer['top']  \n",
    "            print('upsample:',layer_id)  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id + 1  \n",
    "\n",
    "        elif items['type'] == 'shortcut':  \n",
    "            prev_layer_id1 = layer_id + int(items['from'])  \n",
    "            prev_layer_id2 = layer_id - 1  \n",
    "            bottom1 = topnames[prev_layer_id1]  \n",
    "            bottom2= topnames[prev_layer_id2]  \n",
    "            shortcut_layer = OrderedDict()  \n",
    "            shortcut_layer['bottom'] = [bottom1, bottom2]  \n",
    "            if items.has_key('name'):  \n",
    "                shortcut_layer['top'] = items['name']  \n",
    "                shortcut_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                shortcut_layer['top'] = 'layer%d-shortcut' % layer_id  \n",
    "                shortcut_layer['name'] = 'layer%d-shortcut' % layer_id  \n",
    "            shortcut_layer['type'] = 'Eltwise'\n",
    "            eltwise_param = OrderedDict()  \n",
    "            eltwise_param['operation'] = 'SUM'  \n",
    "            shortcut_layer['eltwise_param'] = eltwise_param  \n",
    "            layers.append(shortcut_layer)  \n",
    "            bottom = shortcut_layer['top']  \n",
    "\n",
    "            if items['activation'] != 'linear':  \n",
    "                relu_layer = OrderedDict()  \n",
    "                relu_layer['bottom'] = bottom  \n",
    "                relu_layer['top'] = bottom  \n",
    "                if block.has_key('name'):  \n",
    "                    relu_layer['name'] = '%s-act' % items['name']  \n",
    "                else:  \n",
    "                    relu_layer['name'] = 'layer%d-act' % layer_id  \n",
    "                relu_layer['type'] = 'ReLU'  \n",
    "                if items['activation'] == 'leaky':  \n",
    "                    relu_param = OrderedDict()  \n",
    "                    relu_param['negative_slope'] = '0.1'  \n",
    "                    relu_layer['relu_param'] = relu_param  \n",
    "                layers.append(relu_layer)  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id + 1             \n",
    "\n",
    "        elif items['type'] == 'connected':  \n",
    "            fc_layer = OrderedDict()  \n",
    "            fc_layer['bottom'] = bottom  \n",
    "            if items.has_key('name'):  \n",
    "                fc_layer['top'] = items['name']  \n",
    "                fc_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                fc_layer['top'] = 'layer%d-fc' % layer_id  \n",
    "                fc_layer['name'] = 'layer%d-fc' % layer_id  \n",
    "            fc_layer['type'] = 'InnerProduct'  \n",
    "            fc_param = OrderedDict()  \n",
    "            fc_param['num_output'] = int(items['output'])  \n",
    "            fc_layer['inner_product_param'] = fc_param  \n",
    "            layers.append(fc_layer)  \n",
    "            bottom = fc_layer['top']  \n",
    "\n",
    "            if items['activation'] != 'linear':  \n",
    "                relu_layer = OrderedDict()  \n",
    "                relu_layer['bottom'] = bottom  \n",
    "                relu_layer['top'] = bottom  \n",
    "                if items.has_key('name'):  \n",
    "                    relu_layer['name'] = '%s-act' % items['name']  \n",
    "                else:  \n",
    "                    relu_layer['name'] = 'layer%d-act' % layer_id  \n",
    "                relu_layer['type'] = 'ReLU'  \n",
    "                if items['activation'] == 'leaky':  \n",
    "                    relu_param = OrderedDict()  \n",
    "                    relu_param['negative_slope'] = '0.1'  \n",
    "                    relu_layer['relu_param'] = relu_param  \n",
    "                layers.append(relu_layer)  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id+1  \n",
    "        else:  \n",
    "            print('unknow layer type %s ' % items['type'])  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id + 1  \n",
    "\n",
    "    net_info = OrderedDict()  \n",
    "    net_info['props'] = props  \n",
    "    net_info['layers'] = layers  \n",
    "    return net_info \n",
    "def save_prototxt(net_info, protofile, yolo=False):\n",
    "    fp = open(protofile, 'w')\n",
    "    # whether add double quote\n",
    "    def format_value(value):\n",
    "        #str = u'%s' % value\n",
    "        #if str.isnumeric():\n",
    "        if is_number(value):\n",
    "            return value\n",
    "        elif value == 'true' or value == 'false' or value == 'MAX' or value == 'SUM' or value == 'AVE':\n",
    "            return value\n",
    "        else:\n",
    "            return '\\\"%s\\\"' % value\n",
    "\n",
    "    def print_block(block_info, prefix, indent):\n",
    "        blanks = ''.join([' ']*indent)\n",
    "        print >>fp, '%s%s {' % (blanks, prefix)\n",
    "        for key,value in block_info.items():\n",
    "            if type(value) == OrderedDict:\n",
    "                print_block(value, key, indent+4)\n",
    "            elif type(value) == list:\n",
    "                for v in value:\n",
    "                    print >> fp, '%s    %s: %s' % (blanks, key, format_value(v))\n",
    "            else:\n",
    "                print >> fp, '%s    %s: %s' % (blanks, key, format_value(value))\n",
    "        print >> fp, '%s}' % blanks\n",
    "        \n",
    "    props = net_info['props']\n",
    "    layers = net_info['layers']\n",
    "    print >> fp, 'name: \\\"%s\\\"' % props['name']\n",
    "    print >> fp, 'input: \\\"%s\\\"' % props['input']\n",
    "    print >> fp, 'input_dim: %s' % props['input_dim'][0]\n",
    "    print >> fp, 'input_dim: %s' % props['input_dim'][1]\n",
    "    print >> fp, 'input_dim: %s' % props['input_dim'][2]\n",
    "    print >> fp, 'input_dim: %s' % props['input_dim'][3]\n",
    "    print >> fp, ''\n",
    "    for layer in layers:\n",
    "        if layer['type'] != 'yolo' or yolo == True:\n",
    "            print_block(layer, 'layer', 0)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cfgfile = '/data/github_repos/darknet/ncs/yolov3-tiny-ncs-without-last-maxpool.cfg'\n",
    "cfgfile = '/data/darknet/cfg/yolov3.cfg'\n",
    "#cfg2prototxt(cfgfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['net_1', 'convolutional_2', 'convolutional_3', 'convolutional_4', 'convolutional_5', 'shortcut_6', 'convolutional_7', 'convolutional_8', 'convolutional_9', 'shortcut_10', 'convolutional_11', 'convolutional_12', 'shortcut_13', 'convolutional_14', 'convolutional_15', 'convolutional_16', 'shortcut_17', 'convolutional_18', 'convolutional_19', 'shortcut_20', 'convolutional_21', 'convolutional_22', 'shortcut_23', 'convolutional_24', 'convolutional_25', 'shortcut_26', 'convolutional_27', 'convolutional_28', 'shortcut_29', 'convolutional_30', 'convolutional_31', 'shortcut_32', 'convolutional_33', 'convolutional_34', 'shortcut_35', 'convolutional_36', 'convolutional_37', 'shortcut_38', 'convolutional_39', 'convolutional_40', 'convolutional_41', 'shortcut_42', 'convolutional_43', 'convolutional_44', 'shortcut_45', 'convolutional_46', 'convolutional_47', 'shortcut_48', 'convolutional_49', 'convolutional_50', 'shortcut_51', 'convolutional_52', 'convolutional_53', 'shortcut_54', 'convolutional_55', 'convolutional_56', 'shortcut_57', 'convolutional_58', 'convolutional_59', 'shortcut_60', 'convolutional_61', 'convolutional_62', 'shortcut_63', 'convolutional_64', 'convolutional_65', 'convolutional_66', 'shortcut_67', 'convolutional_68', 'convolutional_69', 'shortcut_70', 'convolutional_71', 'convolutional_72', 'shortcut_73', 'convolutional_74', 'convolutional_75', 'shortcut_76', 'convolutional_77', 'convolutional_78', 'convolutional_79', 'convolutional_80', 'convolutional_81', 'convolutional_82', 'convolutional_83', 'yolo_84', 'route_85', 'convolutional_86', 'upsample_87', 'route_88', 'convolutional_89', 'convolutional_90', 'convolutional_91', 'convolutional_92', 'convolutional_93', 'convolutional_94', 'convolutional_95', 'yolo_96', 'route_97', 'convolutional_98', 'upsample_99', 'route_100', 'convolutional_101', 'convolutional_102', 'convolutional_103', 'convolutional_104', 'convolutional_105', 'convolutional_106', 'convolutional_107', 'yolo_108']\n",
      "{'hue': '.1', 'saturation': '1.5', 'angle': '0', 'decay': '0.0005', 'learning_rate': '0.001', 'scales': '.1,.1', 'batch': '1', 'height': '416', 'channels': '3', 'width': '416', 'subdivisions': '1', 'burn_in': '1000', 'policy': 'steps', 'max_batches': '500200', 'steps': '400000,450000', 'type': 'net', 'momentum': '0.9', 'exposure': '1.5'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '32', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '2', 'pad': '1', 'filters': '64', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '32', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '64', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '2', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '64', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '64', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '2', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '2', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '2', 'pad': '1', 'filters': '1024', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '1024', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '1024', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '1024', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '1024', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'type': 'shortcut', 'activation': 'linear', 'from': '-3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '1024', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '1024', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '1024', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'linear', 'stride': '1', 'pad': '1', 'filters': '255', 'type': 'convolutional', 'size': '1'}\n",
      "{'jitter': '.3', 'anchors': '10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326', 'random': '1', 'mask': '6,7,8', 'num': '9', 'classes': '80', 'ignore_thresh': '.7', 'truth_thresh': '1', 'type': 'yolo'}\n",
      "{'layers': '-4', 'type': 'route'}\n",
      "-4\n",
      "OrderedDict([('bottom', 'layer80-conv'), ('top', 'layer84-route'), ('name', 'layer84-route'), ('type', 'Concat')])\n",
      "84\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'stride': '2', 'type': 'upsample'}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-be53aa564495>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msaved_prototxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcfgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.prototxt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mnet_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg2prototxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfgfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0msave_prototxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_prototxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#darknet2caffe(cfgfile, weightfile, protofile, caffemodel)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-96ebcbaa6922>\u001b[0m in \u001b[0;36mcfg2prototxt\u001b[0;34m(cfgfile)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'upsample'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mupsample_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stride'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m             \u001b[0mupsample_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bottom'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbottom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not str"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':  \n",
    "    '''\n",
    "    cfgfile = sys.argv[1]   \n",
    "    weightfile = sys.argv[2]  \n",
    "    protofile = sys.argv[3]  \n",
    "    caffemodel = sys.argv[4]\n",
    "    '''\n",
    "    import datetime\n",
    "    \n",
    "    saved_prototxt = str(datetime.datetime.now()).split(' ')[-1] + cfgfile.split('/')[0] + '.prototxt'\n",
    "    net_info = cfg2prototxt(cfgfile)    \n",
    "    save_prototxt(net_info, saved_prototxt) \n",
    "    #darknet2caffe(cfgfile, weightfile, protofile, caffemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
