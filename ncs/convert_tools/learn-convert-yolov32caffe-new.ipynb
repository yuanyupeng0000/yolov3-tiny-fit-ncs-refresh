{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fisrt:convert yolov3-tiny cfg to caffe prototxt\n",
    "# Secnd:convert yolov3-tiny weights to caffe caffemodel \n",
    "# Final:use ncs model optimizer to generate corespending xml and bin\n",
    "# Note: For NCS1 to Generate fixed 6classes graph , Please use another file named 'do_convert_ncs1_ncs2_new.py'.\n",
    "# We use NCS1 just 6 classes before , so the code is not fit for other generally classes condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data/ssd-caffe/py2_upsample_caffe/python', '/data/ssd-caffe/py2_caffe/python/', '', '/opt/intel/openvino_2019.2.242/python/python3.5', '/opt/intel/openvino_2019.2.242/python/python3', '/opt/intel/openvino_2019.2.242/deployment_tools/model_optimizer', '/data/github_repos/yolov3-tiny-fit-ncs/ncs/convert_tools', '/data/ssd-caffe/caffe/python', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-x86_64-linux-gnu', '/usr/lib/python2.7/lib-tk', '/usr/lib/python2.7/lib-old', '/usr/lib/python2.7/lib-dynload', '/home/yyp/.local/lib/python2.7/site-packages', '/usr/local/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages/gtk-2.0', '/home/yyp/.local/lib/python2.7/site-packages/IPython/extensions', '/home/yyp/.ipython']\n",
      "['net_1', 'convolutional_2', 'maxpool_3', 'convolutional_4', 'maxpool_5', 'convolutional_6', 'maxpool_7', 'convolutional_8', 'maxpool_9', 'convolutional_10', 'maxpool_11', 'convolutional_12', 'convolutional_13', 'convolutional_14', 'convolutional_15', 'convolutional_16', 'yolo_17', 'route_18', 'convolutional_19', 'deconvolutional_20', 'route_21', 'convolutional_22', 'convolutional_23', 'yolo_24']\n",
      "{'hue': '.1', 'saturation': '1.5', 'angle': '0', 'decay': '0.0005', 'learning_rate': '0.0005', 'scales': '.1,.1', 'batch': '1', 'height': '416', 'channels': '3', 'width': '416', 'subdivisions': '1', 'burn_in': '1000', 'policy': 'steps', 'max_batches': '200200', 'steps': '400000,450000', 'type': 'net', 'momentum': '0.9', 'exposure': '1.5'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '16', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'stride': '2', 'type': 'maxpool', 'size': '2'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '32', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'stride': '2', 'type': 'maxpool', 'size': '2'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '64', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'stride': '2', 'type': 'maxpool', 'size': '2'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'stride': '2', 'type': 'maxpool', 'size': '2'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'stride': '2', 'type': 'maxpool', 'size': '2'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '1024', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'linear', 'stride': '1', 'pad': '1', 'filters': '36', 'type': 'convolutional', 'size': '1'}\n",
      "{'jitter': '.3', 'anchors': '10,25,  20,50,  30,75, 50,125,  80,200,  150,150', 'random': '1', 'mask': '3,4,5', 'num': '6', 'classes': '7', 'ignore_thresh': '.7', 'truth_thresh': '1', 'type': 'yolo'}\n",
      "{'layers': '-4', 'type': 'route'}\n",
      "-4\n",
      "OrderedDict([('bottom', 'layer13-conv'), ('top', 'layer17-route'), ('name', 'layer17-route'), ('type', 'Concat')])\n",
      "17\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'type': 'convolutional', 'size': '1'}\n",
      "{'activation': 'leaky', 'stride': '2', 'pad': '0', 'filters': '128', 'type': 'deconvolutional', 'size': '2'}\n",
      "{'layers': '-1, 8', 'type': 'route'}\n",
      "-1\n",
      "{1: 'layer1-conv', 2: 'layer2-maxpool', 3: 'layer3-conv', 4: 'layer4-maxpool', 5: 'layer5-conv', 6: 'layer6-maxpool', 7: 'layer7-conv', 8: 'layer8-maxpool', 9: 'layer9-conv', 10: 'layer10-maxpool', 11: 'layer11-conv', 12: 'layer12-conv', 13: 'layer13-conv', 14: 'layer14-conv', 15: 'layer15-conv', 17: 'layer17-route', 18: 'layer18-conv', 19: 'layer19-upsample'}\n",
      "OrderedDict([('bottom', ['layer19-upsample', 'layer9-conv']), ('top', 'layer20-route'), ('name', 'layer20-route'), ('type', 'Concat')])\n",
      "20\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'type': 'convolutional', 'size': '3'}\n",
      "{'activation': 'linear', 'stride': '1', 'pad': '1', 'filters': '36', 'type': 'convolutional', 'size': '1'}\n",
      "{'jitter': '.3', 'anchors': '10,25,  20,50,  30,75, 50,125,  80,200,  150,150', 'random': '1', 'mask': '0,1,2', 'num': '6', 'classes': '7', 'ignore_thresh': '.7', 'truth_thresh': '1', 'type': 'yolo'}\n"
     ]
    }
   ],
   "source": [
    "import os, sys, datetime\n",
    "print(sys.path)\n",
    "sys.path.insert(0,'/data/ssd-caffe/py2_caffe/python/')\n",
    "import caffe  \n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from ConfigParser import ConfigParser\n",
    "\n",
    "class UniqDict(OrderedDict):\n",
    "    _unique = 0\n",
    "    def __setitem__(self, key, val):\n",
    "        if isinstance(val, OrderedDict):\n",
    "            self._unique += 1\n",
    "            key += \"_\"+str(self._unique)\n",
    "        OrderedDict.__setitem__(self, key, val)\n",
    "def parse_cfg(cfgfile):\n",
    "    parser = ConfigParser(dict_type=UniqDict)\n",
    "    parser.read(cfgfile)\n",
    "    blocks = parser.sections()\n",
    "    return blocks, parser\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "def cfg2prototxt(cfgfile):\n",
    "    blocks, parser = parse_cfg(cfgfile)\n",
    "    print(blocks)\n",
    "    layers = []  \n",
    "    props = OrderedDict()  \n",
    "    bottom = 'data'  \n",
    "    layer_id = 1  \n",
    "    topnames = dict()  \n",
    "    for block in blocks:\n",
    "        items = dict(parser.items(block))\n",
    "        items['type'] = block.split('_')[0]\n",
    "        print(items)\n",
    "        if items['type'] == 'net':  \n",
    "            props['name'] = 'Darkent2Caffe'  \n",
    "            props['input'] = 'data'  \n",
    "            props['input_dim'] = ['1']  \n",
    "            props['input_dim'].append(items['channels'])  \n",
    "            props['input_dim'].append(items['height'])  \n",
    "            props['input_dim'].append(items['width'])  \n",
    "            continue  \n",
    "        elif (items['type'] == 'convolutional' or items['type'] == 'deconvolutional'):  \n",
    "            conv_layer = OrderedDict()  \n",
    "            conv_layer['bottom'] = bottom  \n",
    "            if items.has_key('name'):  \n",
    "                conv_layer['top'] = items['name']  \n",
    "                conv_layer['name'] = items['name']  \n",
    "            else:\n",
    "                if(items['type'] == 'convolutional'):\n",
    "                    conv_layer['top'] = 'layer%d-conv' % layer_id  \n",
    "                    conv_layer['name'] = 'layer%d-conv' % layer_id\n",
    "                elif(items['type'] == 'deconvolutional'):\n",
    "                    conv_layer['top'] = 'layer%d-upsample' % layer_id  \n",
    "                    conv_layer['name'] = 'layer%d-upsample' % layer_id\n",
    "                    \n",
    "            if(items['type'] == 'deconvolutional'):\n",
    "                conv_layer['type'] = 'Deconvolution'\n",
    "            elif(items['type'] == 'convolutional'):\n",
    "                conv_layer['type'] = 'Convolution' \n",
    "            convolution_param = OrderedDict()  \n",
    "            convolution_param['num_output'] = items['filters']  \n",
    "            convolution_param['kernel_size'] = items['size']  \n",
    "            if items['pad'] == '1':  \n",
    "                convolution_param['pad'] = str(int(convolution_param['kernel_size'])/2)  \n",
    "            convolution_param['stride'] = items['stride']\n",
    "            if items.has_key('batch_normalize'):\n",
    "                if items['batch_normalize'] == '1':  \n",
    "                    convolution_param['bias_term'] = 'false' \n",
    "            else:  \n",
    "                convolution_param['bias_term'] = 'true'  \n",
    "            conv_layer['convolution_param'] = convolution_param  \n",
    "            layers.append(conv_layer)  \n",
    "            bottom = conv_layer['top']  \n",
    "            if items.has_key('batch_normalize'):\n",
    "                if items['batch_normalize'] == '1': \n",
    "                    bn_layer = OrderedDict()  \n",
    "                    bn_layer['bottom'] = bottom  \n",
    "                    bn_layer['top'] = bottom  \n",
    "                    if items.has_key('name'):  \n",
    "                        bn_layer['name'] = '%s-bn' % items['name']  \n",
    "                    else:  \n",
    "                        bn_layer['name'] = 'layer%d-bn' % layer_id  \n",
    "                    bn_layer['type'] = 'BatchNorm'  \n",
    "                    batch_norm_param = OrderedDict()  \n",
    "                    batch_norm_param['use_global_stats'] = 'true'  \n",
    "                    bn_layer['batch_norm_param'] = batch_norm_param  \n",
    "                    layers.append(bn_layer)  \n",
    "\n",
    "                    scale_layer = OrderedDict()  \n",
    "                    scale_layer['bottom'] = bottom  \n",
    "                    scale_layer['top'] = bottom  \n",
    "                    if items.has_key('name'):  \n",
    "                        scale_layer['name'] = '%s-scale' % items['name']  \n",
    "                    else:  \n",
    "                        scale_layer['name'] = 'layer%d-scale' % layer_id  \n",
    "                    scale_layer['type'] = 'Scale'  \n",
    "                    scale_param = OrderedDict()  \n",
    "                    scale_param['bias_term'] = 'true'  \n",
    "                    scale_layer['scale_param'] = scale_param  \n",
    "                    layers.append(scale_layer)  \n",
    "\n",
    "            if items['activation'] != 'linear':  \n",
    "                relu_layer = OrderedDict()  \n",
    "                relu_layer['bottom'] = bottom  \n",
    "                relu_layer['top'] = bottom  \n",
    "                if items.has_key('name'):  \n",
    "                    relu_layer['name'] = '%s-act' % items['name']  \n",
    "                else:  \n",
    "                    relu_layer['name'] = 'layer%d-act' % layer_id  \n",
    "                relu_layer['type'] = 'ReLU'  \n",
    "                if items['activation'] == 'leaky':  \n",
    "                    relu_param = OrderedDict()  \n",
    "                    relu_param['negative_slope'] = '0.1'  \n",
    "                    relu_layer['relu_param'] = relu_param  \n",
    "                layers.append(relu_layer)  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id+1  \n",
    "        elif items['type'] == 'maxpool':  \n",
    "            max_layer = OrderedDict()  \n",
    "            max_layer['bottom'] = bottom  \n",
    "            if items.has_key('name'):  \n",
    "                max_layer['top'] = items['name']  \n",
    "                max_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                max_layer['top'] = 'layer%d-maxpool' % layer_id  \n",
    "                max_layer['name'] = 'layer%d-maxpool' % layer_id  \n",
    "            max_layer['type'] = 'Pooling'  \n",
    "            pooling_param = OrderedDict()  \n",
    "            pooling_param['kernel_size'] = items['size']  \n",
    "            pooling_param['stride'] = items['stride']  \n",
    "            pooling_param['pool'] = 'MAX'  \n",
    "            if items.has_key('pad') and int(items['pad']) == 1:  \n",
    "                pooling_param['pad'] = str((int(items['size'])-1)/2)  \n",
    "            max_layer['pooling_param'] = pooling_param  \n",
    "            layers.append(max_layer)  \n",
    "            bottom = max_layer['top']  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id+1  \n",
    "        elif items['type'] == 'avgpool':  \n",
    "            avg_layer = OrderedDict()  \n",
    "            avg_layer['bottom'] = bottom  \n",
    "            if block.has_key('name'):  \n",
    "                avg_layer['top'] = items['name']  \n",
    "                avg_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                avg_layer['top'] = 'layer%d-avgpool' % layer_id  \n",
    "                avg_layer['name'] = 'layer%d-avgpool' % layer_id  \n",
    "            avg_layer['type'] = 'Pooling'  \n",
    "            pooling_param = OrderedDict()  \n",
    "            pooling_param['kernel_size'] = 7  \n",
    "            pooling_param['stride'] = 1  \n",
    "            pooling_param['pool'] = 'AVE'  \n",
    "            avg_layer['pooling_param'] = pooling_param  \n",
    "            layers.append(avg_layer)  \n",
    "            bottom = avg_layer['top']  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id+1  \n",
    "        elif items['type'] == 'yolo': \n",
    "            layer_id = layer_id + 1\n",
    "            continue\n",
    "            region_layer = OrderedDict()  \n",
    "            region_layer['bottom'] = bottom  \n",
    "            if items.has_key('name'):  \n",
    "                region_layer['top'] = items['name']  \n",
    "                region_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                region_layer['top'] = 'layer%d-yolo' % layer_id  \n",
    "                region_layer['name'] = 'layer%d-yolo' % layer_id  \n",
    "            region_layer['type'] = 'Yolo'  \n",
    "            region_param = OrderedDict()  \n",
    "            region_param['anchors'] = items['anchors'].strip()  \n",
    "            region_param['classes'] = items['classes']  \n",
    "            region_param['num'] = items['num']  \n",
    "            region_layer['yolo_param'] = region_param  \n",
    "            layers.append(region_layer)  \n",
    "            bottom = region_layer['top']  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id + 1\n",
    "\n",
    "        elif items['type'] == 'route':\n",
    "            route_layer = OrderedDict()  \n",
    "            layer_name = str(items['layers']).split(',')  \n",
    "            print(layer_name[0])  \n",
    "            bottom_layer_size = len(str(items['layers']).split(','))  \n",
    "            #print(bottom_layer_size)  \n",
    "            if(1 == bottom_layer_size):  \n",
    "                prev_layer_id = layer_id + int(items['layers'])  \n",
    "                bottom = topnames[prev_layer_id]  \n",
    "                #topnames[layer_id] = bottom  \n",
    "            route_layer['bottom'] = bottom  \n",
    "            if(2 == bottom_layer_size):  \n",
    "                prev_layer_id1 = layer_id + int(layer_name[0])  \n",
    "                #print(prev_layer_id1)  \n",
    "                prev_layer_id2 = int(layer_name[1]) + 1  \n",
    "                print(topnames)  \n",
    "                bottom1 = topnames[prev_layer_id1]  \n",
    "                bottom2 = topnames[prev_layer_id2]  \n",
    "                route_layer['bottom'] = [bottom1, bottom2]  \n",
    "            if items.has_key('name'):  \n",
    "                route_layer['top'] = items['name']  \n",
    "                route_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                route_layer['top'] = 'layer%d-route' % layer_id  \n",
    "                route_layer['name'] = 'layer%d-route' % layer_id  \n",
    "            route_layer['type'] = 'Concat'\n",
    "            print(route_layer)  \n",
    "            layers.append(route_layer)  \n",
    "            bottom = route_layer['top']  \n",
    "            print(layer_id)  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id + 1  \n",
    "\n",
    "        elif items['type'] == 'upsample':\n",
    "            upsample_layer = OrderedDict()  \n",
    "            print(items['stride'])  \n",
    "            upsample_layer['bottom'] = bottom  \n",
    "            if items.has_key('name'):  \n",
    "                upsample_layer['top'] = items['name']  \n",
    "                upsample_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                upsample_layer['top'] = 'layer%d-upsample' % layer_id  \n",
    "                upsample_layer['name'] = 'layer%d-upsample' % layer_id  \n",
    "            upsample_layer['type'] = 'Upsample'  \n",
    "            upsample_param = OrderedDict()  \n",
    "            upsample_param['scale'] = items['stride']  \n",
    "            upsample_layer['upsample_param'] = upsample_param  \n",
    "            print(upsample_layer)  \n",
    "            layers.append(upsample_layer)  \n",
    "            bottom = upsample_layer['top']  \n",
    "            print('upsample:',layer_id)  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id + 1  \n",
    "\n",
    "        elif items['type'] == 'shortcut':  \n",
    "            prev_layer_id1 = layer_id + int(items['from'])  \n",
    "            prev_layer_id2 = layer_id - 1  \n",
    "            bottom1 = topnames[prev_layer_id1]  \n",
    "            bottom2= topnames[prev_layer_id2]  \n",
    "            shortcut_layer = OrderedDict()  \n",
    "            shortcut_layer['bottom'] = [bottom1, bottom2]  \n",
    "            if items.has_key('name'):  \n",
    "                shortcut_layer['top'] = items['name']  \n",
    "                shortcut_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                shortcut_layer['top'] = 'layer%d-shortcut' % layer_id  \n",
    "                shortcut_layer['name'] = 'layer%d-shortcut' % layer_id  \n",
    "            shortcut_layer['type'] = 'Eltwise'\n",
    "            eltwise_param = OrderedDict()  \n",
    "            eltwise_param['operation'] = 'SUM'  \n",
    "            shortcut_layer['eltwise_param'] = eltwise_param  \n",
    "            layers.append(shortcut_layer)  \n",
    "            bottom = shortcut_layer['top']  \n",
    "\n",
    "            if items['activation'] != 'linear':  \n",
    "                relu_layer = OrderedDict()  \n",
    "                relu_layer['bottom'] = bottom  \n",
    "                relu_layer['top'] = bottom  \n",
    "                if block.has_key('name'):  \n",
    "                    relu_layer['name'] = '%s-act' % items['name']  \n",
    "                else:  \n",
    "                    relu_layer['name'] = 'layer%d-act' % layer_id  \n",
    "                relu_layer['type'] = 'ReLU'  \n",
    "                if items['activation'] == 'leaky':  \n",
    "                    relu_param = OrderedDict()  \n",
    "                    relu_param['negative_slope'] = '0.1'  \n",
    "                    relu_layer['relu_param'] = relu_param  \n",
    "                layers.append(relu_layer)  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id + 1             \n",
    "\n",
    "        elif items['type'] == 'connected':  \n",
    "            fc_layer = OrderedDict()  \n",
    "            fc_layer['bottom'] = bottom  \n",
    "            if items.has_key('name'):  \n",
    "                fc_layer['top'] = items['name']  \n",
    "                fc_layer['name'] = items['name']  \n",
    "            else:  \n",
    "                fc_layer['top'] = 'layer%d-fc' % layer_id  \n",
    "                fc_layer['name'] = 'layer%d-fc' % layer_id  \n",
    "            fc_layer['type'] = 'InnerProduct'  \n",
    "            fc_param = OrderedDict()  \n",
    "            fc_param['num_output'] = int(items['output'])  \n",
    "            fc_layer['inner_product_param'] = fc_param  \n",
    "            layers.append(fc_layer)  \n",
    "            bottom = fc_layer['top']  \n",
    "\n",
    "            if items['activation'] != 'linear':  \n",
    "                relu_layer = OrderedDict()  \n",
    "                relu_layer['bottom'] = bottom  \n",
    "                relu_layer['top'] = bottom  \n",
    "                if items.has_key('name'):  \n",
    "                    relu_layer['name'] = '%s-act' % items['name']  \n",
    "                else:  \n",
    "                    relu_layer['name'] = 'layer%d-act' % layer_id  \n",
    "                relu_layer['type'] = 'ReLU'  \n",
    "                if items['activation'] == 'leaky':  \n",
    "                    relu_param = OrderedDict()  \n",
    "                    relu_param['negative_slope'] = '0.1'  \n",
    "                    relu_layer['relu_param'] = relu_param  \n",
    "                layers.append(relu_layer)  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id+1  \n",
    "        else:  \n",
    "            print('unknow layer type %s ' % items['type'])  \n",
    "            topnames[layer_id] = bottom  \n",
    "            layer_id = layer_id + 1  \n",
    "\n",
    "    net_info = OrderedDict()  \n",
    "    net_info['props'] = props  \n",
    "    net_info['layers'] = layers  \n",
    "    return net_info\n",
    "\n",
    "def save_prototxt(net_info, protofile, yolo=False):\n",
    "    fp = open(protofile, 'w')\n",
    "    # whether add double quote\n",
    "    def format_value(value):\n",
    "        #str = u'%s' % value\n",
    "        #if str.isnumeric():\n",
    "        if is_number(value):\n",
    "            return value\n",
    "        elif value == 'true' or value == 'false' or value == 'MAX' or value == 'SUM' or value == 'AVE':\n",
    "            return value\n",
    "        else:\n",
    "            return '\\\"%s\\\"' % value\n",
    "\n",
    "    def print_block(block_info, prefix, indent):\n",
    "        blanks = ''.join([' ']*indent)\n",
    "        print >>fp, '%s%s {' % (blanks, prefix)\n",
    "        for key,value in block_info.items():\n",
    "            if type(value) == OrderedDict:\n",
    "                print_block(value, key, indent+4)\n",
    "            elif type(value) == list:\n",
    "                for v in value:\n",
    "                    print >> fp, '%s    %s: %s' % (blanks, key, format_value(v))\n",
    "            else:\n",
    "                print >> fp, '%s    %s: %s' % (blanks, key, format_value(value))\n",
    "        print >> fp, '%s}' % blanks\n",
    "        \n",
    "    props = net_info['props']\n",
    "    layers = net_info['layers']\n",
    "    print >> fp, 'name: \\\"%s\\\"' % props['name']\n",
    "    print >> fp, 'input: \\\"%s\\\"' % props['input']\n",
    "    print >> fp, 'input_shape{'\n",
    "    print >> fp, '  dim: %s' % props['input_dim'][0]\n",
    "    print >> fp, '  dim: %s' % props['input_dim'][1]\n",
    "    print >> fp, '  dim: %s' % props['input_dim'][2]\n",
    "    print >> fp, '  dim: %s' % props['input_dim'][3]\n",
    "    print >> fp, '}'\n",
    "    #print >> fp, ''\n",
    "    for layer in layers:\n",
    "        if layer['type'] != 'yolo' or yolo == True:\n",
    "            print_block(layer, 'layer', 0)\n",
    "    fp.close()\n",
    "cfgfile = '/data/github_repos/darknet/ncs/yolov3-tiny-ncs-without-last-maxpool.cfg'\n",
    "cfgfile = '/data/github_repos/darknet/ncs/yolov3-tiny-ncs-without-last-maxpool-refined-anchors-7cls-test.cfg'\n",
    "cfgfile = '/data/github_repos/bigfile/yolov3-tiny-ncs-without-last-maxpool-refined-anchors-3cls-extend-\\\n",
    "#remove-maxpooling_new_test_864.cfg'\n",
    "cfgfile = '/media/yyp/DATA/EVALUATE/yolov3-tiny-ncs-without-last-maxpool-refined-anchors-7cls-test.cfg'\n",
    "#cfgfile = '/media/yyp/DATA/EVALUATE/yolov3-tiny-ncs-without-last-maxpool-refined-anchors-7cls-test.cfg'\n",
    "#cfgfile = '/media/yyp/DATA/EVALUATE/yolov3-tiny-ncs-without-last-maxpool-refined-anchors-6cls-new-refine-\\\n",
    "#shortcut-test.cfg'\n",
    "#cfgfile = '/media/yyp/DATA/EVALUATE/yolov3-tiny-ncs-without-last-maxpool-refined-anchors-4cls-test.cfg'\n",
    "saved_prototxt = str(datetime.datetime.now()).split(' ')[-1] + cfgfile.split('/')[-1] + '.prototxt'\n",
    "net_info = cfg2prototxt(cfgfile)    \n",
    "save_prototxt(net_info, saved_prototxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmark\n",
      "[('layer1-conv', [(16, 3, 3, 3)]), ('layer1-bn', [(16,), (16,), (1,)]), ('layer1-scale', [(16,), (16,)]), ('layer3-conv', [(32, 16, 3, 3)]), ('layer3-bn', [(32,), (32,), (1,)]), ('layer3-scale', [(32,), (32,)]), ('layer5-conv', [(64, 32, 3, 3)]), ('layer5-bn', [(64,), (64,), (1,)]), ('layer5-scale', [(64,), (64,)]), ('layer7-conv', [(128, 64, 3, 3)]), ('layer7-bn', [(128,), (128,), (1,)]), ('layer7-scale', [(128,), (128,)]), ('layer9-conv', [(256, 128, 3, 3)]), ('layer9-bn', [(256,), (256,), (1,)]), ('layer9-scale', [(256,), (256,)]), ('layer11-conv', [(512, 256, 3, 3)]), ('layer11-bn', [(512,), (512,), (1,)]), ('layer11-scale', [(512,), (512,)]), ('layer12-conv', [(1024, 512, 3, 3)]), ('layer12-bn', [(1024,), (1024,), (1,)]), ('layer12-scale', [(1024,), (1024,)]), ('layer13-conv', [(256, 1024, 1, 1)]), ('layer13-bn', [(256,), (256,), (1,)]), ('layer13-scale', [(256,), (256,)]), ('layer14-conv', [(512, 256, 3, 3)]), ('layer14-bn', [(512,), (512,), (1,)]), ('layer14-scale', [(512,), (512,)]), ('layer15-conv', [(36, 512, 1, 1), (36,)]), ('layer18-conv', [(128, 256, 1, 1)]), ('layer18-bn', [(128,), (128,), (1,)]), ('layer18-scale', [(128,), (128,)]), ('layer19-upsample', [(128, 128, 2, 2), (128,)]), ('layer21-conv', [(256, 384, 3, 3)]), ('layer21-bn', [(256,), (256,), (1,)]), ('layer21-scale', [(256,), (256,)]), ('layer22-conv', [(36, 256, 1, 1), (36,)])]\n",
      "benchmark\n",
      "benchmark\n",
      "['net_1', 'convolutional_2', 'maxpool_3', 'convolutional_4', 'maxpool_5', 'convolutional_6', 'maxpool_7', 'convolutional_8', 'maxpool_9', 'convolutional_10', 'maxpool_11', 'convolutional_12', 'convolutional_13', 'convolutional_14', 'convolutional_15', 'convolutional_16', 'yolo_17', 'route_18', 'convolutional_19', 'deconvolutional_20', 'route_21', 'convolutional_22', 'convolutional_23', 'yolo_24']\n",
      "[      0       2       0 6400000       0]\n",
      "buf len:8755768\n",
      "net_1\n",
      "{'hue': '.1', 'saturation': '1.5', 'angle': '0', 'decay': '0.0005', 'learning_rate': '0.0005', 'scales': '.1,.1', 'batch': '1', 'height': '416', 'channels': '3', 'width': '416', 'subdivisions': '1', 'burn_in': '1000', 'policy': 'steps', 'max_batches': '200200', 'steps': '400000,450000', 'momentum': '0.9', 'exposure': '1.5'}\n",
      "convolutional_2\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '16', 'batch_normalize': '1', 'size': '3'}\n",
      "has no name layer1-conv\n",
      "load_conv_bn2caffe:\n",
      "start:496\n",
      "maxpool_3\n",
      "{'stride': '2', 'size': '2'}\n",
      "convolutional_4\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '32', 'batch_normalize': '1', 'size': '3'}\n",
      "has no name layer3-conv\n",
      "load_conv_bn2caffe:\n",
      "start:5232\n",
      "maxpool_5\n",
      "{'stride': '2', 'size': '2'}\n",
      "convolutional_6\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '64', 'batch_normalize': '1', 'size': '3'}\n",
      "has no name layer5-conv\n",
      "load_conv_bn2caffe:\n",
      "start:23920\n",
      "maxpool_7\n",
      "{'stride': '2', 'size': '2'}\n",
      "convolutional_8\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'size': '3'}\n",
      "has no name layer7-conv\n",
      "load_conv_bn2caffe:\n",
      "start:98160\n",
      "maxpool_9\n",
      "{'stride': '2', 'size': '2'}\n",
      "convolutional_10\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'size': '3'}\n",
      "has no name layer9-conv\n",
      "load_conv_bn2caffe:\n",
      "start:394096\n",
      "maxpool_11\n",
      "{'stride': '2', 'size': '2'}\n",
      "convolutional_12\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'size': '3'}\n",
      "has no name layer11-conv\n",
      "load_conv_bn2caffe:\n",
      "start:1575792\n",
      "convolutional_13\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '1024', 'batch_normalize': '1', 'size': '3'}\n",
      "has no name layer12-conv\n",
      "load_conv_bn2caffe:\n",
      "start:6298480\n",
      "convolutional_14\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'size': '1'}\n",
      "has no name layer13-conv\n",
      "load_conv_bn2caffe:\n",
      "start:6561648\n",
      "convolutional_15\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'size': '3'}\n",
      "has no name layer14-conv\n",
      "load_conv_bn2caffe:\n",
      "start:7743344\n",
      "convolutional_16\n",
      "{'stride': '1', 'activation': 'linear', 'pad': '1', 'filters': '36', 'size': '1'}\n",
      "has no name layer15-conv\n",
      "load_conv2caffe:\n",
      "start:7761812\n",
      "yolo_17\n",
      "{'jitter': '.3', 'anchors': '10,25,  20,50,  30,75, 50,125,  80,200,  150,150', 'random': '1', 'mask': '3,4,5', 'num': '6', 'classes': '7', 'ignore_thresh': '.7', 'truth_thresh': '1'}\n",
      "unknow layer type yolo \n",
      "route_18\n",
      "{'layers': '-4'}\n",
      "convolutional_19\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'size': '1'}\n",
      "has no name layer18-conv\n",
      "load_conv_bn2caffe:\n",
      "start:7795092\n",
      "deconvolutional_20\n",
      "{'stride': '2', 'activation': 'leaky', 'pad': '0', 'filters': '128', 'size': '2'}\n",
      "has no name layer19-upsample\n",
      "load_conv2caffe:\n",
      "start:7860756\n",
      "route_21\n",
      "{'layers': '-1, 8'}\n",
      "convolutional_22\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'size': '3'}\n",
      "has no name layer21-conv\n",
      "load_conv_bn2caffe:\n",
      "start:8746516\n",
      "convolutional_23\n",
      "{'stride': '1', 'activation': 'linear', 'pad': '1', 'filters': '36', 'size': '1'}\n",
      "has no name layer22-conv\n",
      "load_conv2caffe:\n",
      "start:8755768\n",
      "yolo_24\n",
      "save caffemodel to 20200525110719_TinyYoloV3NCS.caffemodel\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "if(len(sys.argv) < 2):\n",
    "    print('please input yolov3-tiny-ncs-weights file')\n",
    "    exit(1)\n",
    "weights = sys.argv[1]\n",
    "'''\n",
    "#weights = '/media/yyp/DATA/EVALUATE/yolov3-tiny-ncs-without-last-maxpool-refined-anchors_150000.weights'\n",
    "weights = '/media/yyp/DATA/EVALUATE/yolov3-tiny-ncs-without-last-maxpool-refined-anchors-7cls_130000.weights'\n",
    "#weights = '/data/github_repos/bigfile/yolov3-tiny-ncs-without-last-maxpool-refined-anchors-3cls-extend-\\\n",
    "#remove-maxpooling_new.backup'\n",
    "#weights = '/media/yyp/DATA/EVALUATE/yolov3-tiny-ncs-without-last-maxpool-refined-anchors-4cls-new-\\\n",
    "#refine-shortcut.backup'\n",
    "#weights = '/media/yyp/DATA/EVALUATE/yolov3-tiny-ncs-without-last-maxpool-refined-anchors-7cls-haikou-person.backup'\n",
    "#weights = '/media/yyp/DATA/EVALUATE/yolov3-tiny-ncs-without-last-maxpool-refined-anchors-6cls-new-refine-\\\n",
    "#shortcut_18000.weights'\n",
    "#weights = '/media/yyp/DATA/EVALUATE/yolov3-tiny-ncs-without-last-maxpool-refined-anchors-4cls_100000.weights'\n",
    "#weights = '/nfsroot/yyp-darknet/darknet/zeshida_weights_20200523/\\\n",
    "#yolov3-tiny-ncs-without-last-maxpool-refined-anchors-4cls.backup'\n",
    "weights = '/nfsroot/yyp-darknet/darknet/yolov3_tiny_20200525_weights_lsw/\\\n",
    "yolov3-tiny-ncs-without-last-maxpool-refined-anchors-7cls_100000.weights'\n",
    "#sys.path.insert(0, '/data/ssd-caffe/py2_caffe/python') \n",
    "#sys.path.insert(0, '/data/ssd-caffe/new-yolov3-caffe/python')\n",
    "sys.path.insert(0, '/data/ssd-caffe/py2_upsample_caffe/python')\n",
    "del caffe\n",
    "import caffe  \n",
    "import numpy as np  \n",
    "from collections import OrderedDict\n",
    "from ConfigParser import ConfigParser\n",
    "\n",
    "#from ConfigParser import ConfigParser\n",
    "class uniqdict(OrderedDict):\n",
    "    _unique = 0\n",
    "    def __setitem__(self, key, val):\n",
    "        if isinstance(val, OrderedDict):\n",
    "            self._unique += 1\n",
    "            key += \"_\"+str(self._unique)\n",
    "        OrderedDict.__setitem__(self, key, val)\n",
    "def load_conv2caffe(buf, start, conv_param):  \n",
    "    weight = conv_param[0].data  \n",
    "    bias = conv_param[1].data  \n",
    "    conv_param[1].data[...] = np.reshape(buf[start:start+bias.size], bias.shape);   start = start + bias.size  \n",
    "    conv_param[0].data[...] = np.reshape(buf[start:start+weight.size], weight.shape); start = start + weight.size  \n",
    "    return start\n",
    "def load_fc2caffe(buf, start, fc_param):  \n",
    "    weight = fc_param[0].data  \n",
    "    bias = fc_param[1].data  \n",
    "    fc_param[1].data[...] = np.reshape(buf[start:start+bias.size], bias.shape);   start = start + bias.size  \n",
    "    fc_param[0].data[...] = np.reshape(buf[start:start+weight.size], weight.shape); start = start + weight.size  \n",
    "    return start\n",
    "def load_conv_bn2caffe(buf, start, conv_param, bn_param, scale_param): \n",
    "    conv_weight = conv_param[0].data  \n",
    "    running_mean = bn_param[0].data  \n",
    "    running_var = bn_param[1].data  \n",
    "    scale_weight = scale_param[0].data  \n",
    "    scale_bias = scale_param[1].data      \n",
    "    scale_param[1].data[...] = np.reshape(buf[start:start+scale_bias.size], scale_bias.shape); start = start + scale_bias.size  \n",
    "    #print scale_bias.size  \n",
    "    #print scale_bias  \n",
    "  \n",
    "    scale_param[0].data[...] = np.reshape(buf[start:start+scale_weight.size], scale_weight.shape); start = start + scale_weight.size  \n",
    "    #print scale_weight.size\n",
    "    #print scale_weight\n",
    "  \n",
    "    bn_param[0].data[...] = np.reshape(buf[start:start+running_mean.size], running_mean.shape); start = start + running_mean.size  \n",
    "    #print running_mean.size\n",
    "    #print running_mean\n",
    "  \n",
    "    bn_param[1].data[...] = np.reshape(buf[start:start+running_var.size], running_var.shape); start = start + running_var.size  \n",
    "    #print running_var.size\n",
    "    #print running_var\n",
    "  \n",
    "    bn_param[2].data[...] = np.array([1.0])  \n",
    "    conv_param[0].data[...] = np.reshape(buf[start:start+conv_weight.size], conv_weight.shape); start = start + conv_weight.size  \n",
    "    #print conv_weight.size\n",
    "    #print conv_weight\n",
    "  \n",
    "    return start\n",
    "def darknet2caffe(cfgfile, weightfile, protofile, caffemodel='gene.caffemodel'):  \n",
    "    #net_info = cfg2prototxt(cfgfile)\n",
    "    #save_prototxt(net_info , protofile, region=False)  \n",
    "    print('benchmark')\n",
    "    net = caffe.Net(protofile, caffe.TEST)\n",
    "    k_v_s = [(k, v) for k, v in net.params.items()]\n",
    "    key_vecnums = [(vecs[0], len(vecs[1])) for vecs in k_v_s]\n",
    "    print([(vecs[0], [vecs[1][i].data.shape for i in range(len(vecs[1]))])for vecs in k_v_s])\n",
    "    print('benchmark')\n",
    "    params = net.params\n",
    "    print('benchmark')\n",
    "  \n",
    "    #blocks = parse_cfg(cfgfile)\n",
    "    parser = ConfigParser(dict_type=uniqdict)\n",
    "    parser.read(cfgfile)\n",
    "    blocks = parser.sections()\n",
    "    print(blocks)\n",
    "    \n",
    "  \n",
    "    #Open the weights file  \n",
    "    fp = open(weightfile, \"rb\")  \n",
    "  \n",
    "    #The first 4 values are header information   \n",
    "    # 1. Major version number  \n",
    "    # 2. Minor Version Number  \n",
    "    # 3. Subversion number   \n",
    "    # 4. IMages seen   \n",
    "    header = np.fromfile(fp, dtype = np.int32, count = 5)\n",
    "    #header = np.fromfile(fp, dtype = np.float32, count = 5)\n",
    "    print(header)\n",
    "    #fp = open(weightfile, 'rb')  \n",
    "    #header = np.fromfile(fp, count=5, dtype=np.int32)  \n",
    "    #header = np.ndarray(shape=(5,),dtype='int32',buffer=fp.read(20))  \n",
    "    #print(header)  \n",
    "    buf = np.fromfile(fp, dtype = np.float32)\n",
    "    print('buf len:{0}'.format(len(buf)))\n",
    "    #print(buf)  \n",
    "    fp.close()  \n",
    "  \n",
    "    layers = []  \n",
    "    layer_id = 1  \n",
    "    start = 0  \n",
    "    for block in blocks:\n",
    "        print(block)\n",
    "        if start >= buf.size:  \n",
    "            break\n",
    "        items = dict(parser.items(block))\n",
    "        print(items)\n",
    "        if block.split('_')[0] == 'net':  \n",
    "            continue\n",
    "        elif ((block.split('_')[0] == 'convolutional') or \n",
    "        (block.split('_')[0] == 'deconvolutional')):\n",
    "            batchnorm_followed = False\n",
    "            relu_followed = False\n",
    "            \n",
    "            if 'batch_normalize' in items and items['batch_normalize']:\n",
    "                batchnorm_followed = True\n",
    "            if 'activation' in items and items['activation'] != 'linear':\n",
    "                relu_followed = True\n",
    "            \n",
    "            if items.has_key('name'):  \n",
    "                conv_layer_name = items['name']  \n",
    "                print('has key name ' + conv_layer_name)\n",
    "                bn_layer_name = '%s-bn' % items['name']  \n",
    "                scale_layer_name = '%s-scale' % items['name']  \n",
    "            else:\n",
    "                if(block.split('_')[0] == 'deconvolutional'):\n",
    "                    conv_layer_name = 'layer%d-upsample' % layer_id  \n",
    "                    print('has no name ' + conv_layer_name)\n",
    "                    #bn_layer_name = 'layer%d-bn' % layer_id  \n",
    "                    #scale_layer_name = 'layer%d-scale' % layer_id \n",
    "                else:\n",
    "                    conv_layer_name = 'layer%d-conv' % layer_id  \n",
    "                    print('has no name ' + conv_layer_name)\n",
    "                    bn_layer_name = 'layer%d-bn' % layer_id  \n",
    "                    scale_layer_name = 'layer%d-scale' % layer_id  \n",
    "  \n",
    "            if batchnorm_followed:\n",
    "                print(\"load_conv_bn2caffe:\")\n",
    "                start = load_conv_bn2caffe(buf, start, params[conv_layer_name], \n",
    "                                           params[bn_layer_name], params[scale_layer_name])\n",
    "            else:\n",
    "                print(\"load_conv2caffe:\")\n",
    "                start = load_conv2caffe(buf, start, params[conv_layer_name])\n",
    "            '''\n",
    "            if(layer_id == 11):\n",
    "                print('layer_id == 11')\n",
    "                layer_id = layer_id + 2\n",
    "            else:\n",
    "                layer_id = layer_id+1\n",
    "            '''\n",
    "            layer_id = layer_id+1\n",
    "            print('start:{0}'.format(start))\n",
    "        elif block.split('_')[0] == 'connected':  \n",
    "            if items.has_key('name'):  \n",
    "                fc_layer_name = items['name']  \n",
    "            else:  \n",
    "                fc_layer_name = 'layer%d-fc' % layer_id  \n",
    "            start = load_fc2caffe(buf, start, params[fc_layer_name])  \n",
    "            layer_id = layer_id+1  \n",
    "        elif block.split('_')[0] == 'maxpool':  \n",
    "            layer_id = layer_id+1  \n",
    "        elif block.split('_')[0] == 'avgpool':  \n",
    "            layer_id = layer_id+1  \n",
    "        elif block.split('_')[0] == 'region':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'route':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'shortcut':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'softmax':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'cost':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'upsample':  \n",
    "            layer_id = layer_id + 1 \n",
    "        else:  \n",
    "            print('unknow layer type %s ' % block.split('_')[0]) \n",
    "            layer_id = layer_id + 1 \n",
    "    print('save caffemodel to %s' % caffemodel)  \n",
    "    net.save(caffemodel)\n",
    "    \n",
    "#cfgfile = '/data/github_repos/yolov3-tiny-fit-ncs/ncs/yolov3-tiny-ncs-without-last-maxpool.cfg'\n",
    "#weights = weights\n",
    "prototxt = '/data/github_repos/yolov3-tiny-fit-ncs/ncs/yolov3-tiny-ncs-without-last-maxpool.prototxt'\n",
    "prototxt = saved_prototxt\n",
    "prototxt1 = '/data/github_repos/yolov3-tiny-fit-ncs/ncs/convert_tools/07:30:49.817376yolov3-tiny-ncs-without-last-maxpool.cfg.prototxt'\n",
    "#caffemodel = 'Jenerated_nolastpooling.caffemodel'\n",
    "caffemodel = (datetime.datetime.now()).strftime(\"%Y%m%d%H%M%S_\") + 'TinyYoloV3NCS.caffemodel'\n",
    "darknet2caffe(cfgfile, weights, prototxt, caffemodel)\n",
    "make_prototxt_command = 'cp ' + prototxt +' ' + caffemodel[:-10] + 'prototxt'\n",
    "prototxt = caffemodel[:-10] + 'prototxt'\n",
    "os.system(make_prototxt_command)\n",
    "#mvnc_command = 'bash generate-graph.sh ' + caffemodel + ' ' + prototxt1\n",
    "#os.system(mvnc_command)\n",
    "set_ncs2_env_command = 'bash /opt/intel/openvino/bin/setupvars.sh'\n",
    "#set_ncs2_env_command = 'bash /opt/intel/computer_vision_sdk/bin/setupvars.sh'\n",
    "os.system(set_ncs2_env_command)\n",
    "mo_command = 'sh /data/github_repos/yolov3-tiny-fit-ncs/ncs2/OpenVINO/model_optimizer/yolov3-tiny-mo.sh ' + '/data/github_repos/yolov3-tiny-fit-ncs/ncs/convert_tools/' + caffemodel\n",
    "#mo_command = 'sh /data/github_repos/yolov3-tiny-fit-ncs/ncs2/OpenVINO/model_optimizer/yolov3-tiny-mo-2018R5.sh ' + '/data/github_repos/yolov3-tiny-fit-ncs/ncs/convert_tools/' + caffemodel\n",
    "\n",
    "os.system(mo_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
