{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConfigParser import ConfigParser\n",
    "from collections import OrderedDict\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaffeLayerGenerator(object):\n",
    "    def __init__(self, name, ltype):\n",
    "        self.name = name\n",
    "        self.bottom = []\n",
    "        self.top = []\n",
    "        self.type = ltype\n",
    "    def get_template(self):\n",
    "        return \"\"\"\n",
    "layer {{{{\n",
    "  name: \"{}\"\n",
    "  type: \"{}\"\n",
    "  bottom: \"{}\"\n",
    "  top: \"{}\"{{}}\n",
    "}}}}\"\"\".format(self.name, self.type, self.bottom[0], self.top[0])\n",
    "class CaffeInputLayer(CaffeLayerGenerator):\n",
    "    def __init__(self, name, channels, width, height):\n",
    "        super(CaffeInputLayer, self).__init__(name, 'Input')\n",
    "        self.channels = channels\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "    def write(self, f):\n",
    "        f.write(\"\"\"\n",
    "input: \"{}\"\n",
    "input_shape {{\n",
    "  dim: 1\n",
    "  dim: {}\n",
    "  dim: {}\n",
    "  dim: {}\n",
    "}}\"\"\".format(self.name, self.channels, self.width, self.height))\n",
    "\n",
    "class CaffeConvolutionLayer(CaffeLayerGenerator):\n",
    "    def __init__(self, name, filters, ksize=None, stride=None, pad=None, bias=True):\n",
    "        super(CaffeConvolutionLayer, self).__init__(name, 'Convolution')\n",
    "        self.filters = filters\n",
    "        self.ksize = ksize\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.bias = bias\n",
    "    def write(self, f):\n",
    "        opts = ['']\n",
    "        if self.ksize is not None: opts.append('kernel_size: {}'.format(self.ksize))\n",
    "        if self.stride is not None: opts.append('stride: {}'.format(self.stride))\n",
    "        if self.pad is not None: opts.append('pad: {}'.format(self.pad))\n",
    "        if not self.bias: opts.append('bias_term: false')\n",
    "        param_str = \"\"\"\n",
    "  convolution_param {{\n",
    "    num_output: {}{}\n",
    "  }}\"\"\".format(self.filters, '\\n    '.join(opts))\n",
    "        f.write(self.get_template().format(param_str))\n",
    "\n",
    "class CaffePoolingLayer(CaffeLayerGenerator):\n",
    "    def __init__(self, name, pooltype, ksize=None, stride=None, pad=None, global_pooling=None):\n",
    "        super(CaffePoolingLayer, self).__init__(name, 'Pooling')\n",
    "        self.pooltype = pooltype\n",
    "        self.ksize = ksize\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.global_pooling = global_pooling\n",
    "    def write(self, f):\n",
    "        opts = ['']\n",
    "        if self.ksize is not None: opts.append('kernel_size: {}'.format(self.ksize))\n",
    "        if self.stride is not None: opts.append('stride: {}'.format(self.stride))\n",
    "        if self.pad is not None: opts.append('pad: {}'.format(self.pad))\n",
    "        if self.global_pooling is not None: opts.append('global_pooling: {}'.format('True' if self.global_pooling else 'False'))\n",
    "        param_str = \"\"\"\n",
    "  pooling_param {{\n",
    "    pool: {}{}\n",
    "  }}\"\"\".format(self.pooltype, '\\n    '.join(opts))\n",
    "        f.write(self.get_template().format(param_str))\n",
    "\n",
    "class CaffeInnerProductLayer(CaffeLayerGenerator):\n",
    "    def __init__(self, name, num_output):\n",
    "        super(CaffeInnerProductLayer, self).__init__(name, 'InnerProduct')\n",
    "        self.num_output = num_output\n",
    "    def write(self, f):\n",
    "        param_str = \"\"\"\n",
    "  inner_product_param {{\n",
    "    num_output: {}\n",
    "  }}\"\"\".format(self.num_output)\n",
    "        f.write(self.get_template().format(param_str))\n",
    "\n",
    "class CaffeBatchNormLayer(CaffeLayerGenerator):\n",
    "    def __init__(self, name):\n",
    "        super(CaffeBatchNormLayer, self).__init__(name, 'BatchNorm')\n",
    "    def write(self, f):\n",
    "        param_str = \"\"\"\n",
    "  batch_norm_param {\n",
    "    use_global_stats: true\n",
    "  }\"\"\"\n",
    "        f.write(self.get_template().format(param_str))\n",
    "\n",
    "class CaffeScaleLayer(CaffeLayerGenerator):\n",
    "    def __init__(self, name):\n",
    "        super(CaffeScaleLayer, self).__init__(name, 'Scale')\n",
    "    def write(self, f):\n",
    "        param_str = \"\"\"\n",
    "  scale_param {\n",
    "    bias_term: true\n",
    "  }\"\"\"\n",
    "        f.write(self.get_template().format(param_str))\n",
    "\n",
    "class CaffeReluLayer(CaffeLayerGenerator):\n",
    "    def __init__(self, name, negslope=None):\n",
    "        super(CaffeReluLayer, self).__init__(name, 'ReLU')\n",
    "        self.negslope = negslope\n",
    "    def write(self, f):\n",
    "        param_str = \"\"\n",
    "        if self.negslope is not None:\n",
    "            param_str = \"\"\"\n",
    "  relu_param {{\n",
    "    negative_slope: {}\n",
    "  }}\"\"\".format(self.negslope)\n",
    "        f.write(self.get_template().format(param_str))\n",
    "\n",
    "class CaffeDropoutLayer(CaffeLayerGenerator):\n",
    "    def __init__(self, name, prob):\n",
    "        super(CaffeDropoutLayer, self).__init__(name, 'Dropout')\n",
    "        self.prob = prob\n",
    "    def write(self, f):\n",
    "        param_str = \"\"\"\n",
    "  dropout_param {{\n",
    "    dropout_ratio: {}\n",
    "  }}\"\"\".format(self.prob)\n",
    "        f.write(self.get_template().format(param_str))\n",
    "\n",
    "class CaffeSoftmaxLayer(CaffeLayerGenerator):\n",
    "    def __init__(self, name):\n",
    "        super(CaffeSoftmaxLayer, self).__init__(name, 'Softmax')\n",
    "    def write(self, f):\n",
    "        f.write(self.get_template().format(\"\"))\n",
    "'''\n",
    "class CaffeConcatLayer(CaffeLayerGenerator):\n",
    "    def __init__(self, name):\n",
    "        super(CaffeConcatLayer, self).__init__(name, 'Concat')\n",
    "    def write(self, f):\n",
    "        f.write(self.get_template().format(\"\"))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaffeProtoGenerator:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.sections = []\n",
    "        self.lnum = 0\n",
    "        self.layer = None\n",
    "    def add_layer(self, l):\n",
    "        self.sections.append( l )\n",
    "    def add_input_layer(self, items):\n",
    "        self.lnum = 0\n",
    "        lname = \"data\"\n",
    "        self.layer = CaffeInputLayer(lname, items['channels'], items['width'], items['height'])\n",
    "        self.layer.top.append( lname )\n",
    "        self.add_layer( self.layer )\n",
    "    def update_last_convolution_layer(self):\n",
    "        self.sections[len(self.sections)-1].pad = 0\n",
    "    def add_convolution_layer(self, items):\n",
    "        self.lnum += 1\n",
    "        prev_blob = self.layer.top[0]\n",
    "        lname = \"conv\"+str(self.lnum)\n",
    "        filters = items['filters']\n",
    "        ksize = items['size'] if 'size' in items else None\n",
    "        stride = items['stride'] if 'stride' in items else None\n",
    "        pad = items['pad'] if 'pad' in items else None\n",
    "        bias = not bool(items['batch_normalize']) if 'batch_normalize' in items else True\n",
    "        self.layer = CaffeConvolutionLayer( lname, filters, ksize=ksize, stride=stride, pad=pad, bias=bias )\n",
    "        self.layer.bottom.append( prev_blob )\n",
    "        self.layer.top.append( lname )\n",
    "        self.add_layer( self.layer )\n",
    "    def add_innerproduct_layer(self, items):\n",
    "        self.lnum += 1\n",
    "        prev_blob = self.layer.top[0]\n",
    "        lname = \"fc\"+str(self.lnum)\n",
    "        num_output = items['output']\n",
    "        self.layer = CaffeInnerProductLayer( lname, num_output )\n",
    "        self.layer.bottom.append( prev_blob )\n",
    "        self.layer.top.append( lname )\n",
    "        self.add_layer( self.layer )\n",
    "    def add_pooling_layer(self, ltype, items, global_pooling=None):\n",
    "        prev_blob = self.layer.top[0]\n",
    "        lname = \"pool\"+str(self.lnum)\n",
    "        ksize = items['size'] if 'size' in items else None\n",
    "        stride = items['stride'] if 'stride' in items else None\n",
    "        pad = items['pad'] if 'pad' in items else None\n",
    "        self.layer = CaffePoolingLayer( lname, ltype, ksize=ksize, stride=stride, pad=pad, global_pooling=global_pooling )\n",
    "        self.layer.bottom.append( prev_blob )\n",
    "        self.layer.top.append( lname )\n",
    "        self.add_layer( self.layer )\n",
    "    def add_batchnorm_layer(self, items):\n",
    "        prev_blob = self.layer.top[0]\n",
    "        lname = \"bn\"+str(self.lnum)\n",
    "        self.layer = CaffeBatchNormLayer( lname )\n",
    "        self.layer.bottom.append( prev_blob )\n",
    "        self.layer.top.append( lname )\n",
    "        self.add_layer( self.layer )\n",
    "    def add_scale_layer(self, items):\n",
    "        prev_blob = self.layer.top[0]\n",
    "        lname = \"scale\"+str(self.lnum)\n",
    "        self.layer = CaffeScaleLayer( lname )\n",
    "        self.layer.bottom.append( prev_blob )\n",
    "        self.layer.top.append( lname )\n",
    "        self.add_layer( self.layer )\n",
    "    def add_relu_layer(self, items):\n",
    "        prev_blob = self.layer.top[0]\n",
    "        lname = \"relu\"+str(self.lnum)\n",
    "        self.layer = CaffeReluLayer( lname , 0.1)\n",
    "        self.layer.bottom.append( prev_blob )\n",
    "        self.layer.top.append( prev_blob )     # loopback\n",
    "        self.add_layer( self.layer )\n",
    "    def add_dropout_layer(self, items):\n",
    "        prev_blob = self.layer.top[0]\n",
    "        lname = \"drop\"+str(self.lnum)\n",
    "        self.layer = CaffeDropoutLayer( lname, items['probability'] )\n",
    "        self.layer.bottom.append( prev_blob )\n",
    "        self.layer.top.append( prev_blob )     # loopback\n",
    "        self.add_layer( self.layer )\n",
    "    def add_softmax_layer(self, items):\n",
    "        prev_blob = self.layer.top[0]\n",
    "        lname = \"prob\"\n",
    "        self.layer = CaffeSoftmaxLayer( lname )\n",
    "        self.layer.bottom.append( prev_blob )\n",
    "        self.layer.top.append( lname )\n",
    "        self.add_layer( self.layer )\n",
    "    '''\n",
    "    def add_concat_layer(self, items):\n",
    "        prev_blob = self.layer.top[0]\n",
    "        lname = \"concat\"\n",
    "        self.layer = CaffeConcatLayer()\n",
    "    '''\n",
    "    def finalize(self, name):\n",
    "        self.layer.top[0] = name    # replace\n",
    "    def write(self, fname):\n",
    "        with open(fname, 'w') as f:\n",
    "            f.write('name: \"{}\"'.format(self.name))\n",
    "            for sec in self.sections:\n",
    "                sec.write(f)\n",
    "        logging.info('{} is generated'.format(fname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class uniqdict(OrderedDict):\n",
    "    _unique = 0\n",
    "    def __setitem__(self, key, val):\n",
    "        if isinstance(val, OrderedDict):\n",
    "            self._unique += 1\n",
    "            key += \"_\"+str(self._unique)\n",
    "        OrderedDict.__setitem__(self, key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(cfgfile, ptxtfile):\n",
    "    #\n",
    "    parser = ConfigParser(dict_type=uniqdict)\n",
    "    parser.read(cfgfile)\n",
    "    netname = os.path.basename(cfgfile).split('.')[0]\n",
    "    #print netname\n",
    "    gen = CaffeProtoGenerator(netname)\n",
    "    for section in parser.sections():\n",
    "        print section\n",
    "        _section = section.split('_')[0]\n",
    "        if _section in [\"crop\", \"cost\"]:\n",
    "            continue\n",
    "        #\n",
    "        batchnorm_followed = False\n",
    "        relu_followed = False\n",
    "        items = dict(parser.items(section))\n",
    "        print items\n",
    "        if 'batch_normalize' in items and items['batch_normalize']:\n",
    "            batchnorm_followed = True\n",
    "        if 'activation' in items and items['activation'] != 'linear':\n",
    "            relu_followed = True\n",
    "        #\n",
    "        if _section == 'net':\n",
    "            gen.add_input_layer(items)\n",
    "        elif _section == 'convolutional':\n",
    "            gen.add_convolution_layer(items)\n",
    "            if batchnorm_followed:\n",
    "                gen.add_batchnorm_layer(items)\n",
    "                gen.add_scale_layer(items)\n",
    "            if relu_followed:\n",
    "                gen.add_relu_layer(items)\n",
    "        elif _section == 'connected':\n",
    "            gen.add_innerproduct_layer(items)\n",
    "            if relu_followed:\n",
    "                gen.add_relu_layer(items)\n",
    "        elif _section == 'maxpool':\n",
    "            gen.add_pooling_layer('MAX', items)\n",
    "        elif _section == 'avgpool':\n",
    "            gen.add_pooling_layer('AVE', items, global_pooling=True)\n",
    "        elif _section == 'dropout':\n",
    "            gen.add_dropout_layer(items)\n",
    "        elif _section == 'softmax':\n",
    "            gen.add_softmax_layer(items)\n",
    "        '''\n",
    "        elif _section == 'route':\n",
    "            gen.add_concat_layer(items)\n",
    "        elif _section == 'deconvolutional':\n",
    "            gen.add_deconvolutional_layer(items)\n",
    "        '''\n",
    "        else:\n",
    "            logging.error(\"{} layer is not supported\".format(_section))\n",
    "    gen.update_last_convolution_layer()\n",
    "    #gen.finalize('result')\n",
    "    gen.write(ptxtfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:yolo layer is not supported\n",
      "ERROR:root:route layer is not supported\n",
      "ERROR:root:deconvolutional layer is not supported\n",
      "ERROR:root:route layer is not supported\n",
      "ERROR:root:yolo layer is not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net_1\n",
      "{'hue': '.1', 'saturation': '1.5', 'angle': '0', 'decay': '0.0005', 'learning_rate': '0.001', 'scales': '.1,.1', 'batch': '1', 'height': '416', 'channels': '3', 'width': '416', 'subdivisions': '1', 'burn_in': '1000', 'policy': 'steps', 'max_batches': '500200', 'steps': '400000,450000', 'momentum': '0.9', 'exposure': '1.5'}\n",
      "convolutional_2\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '16', 'batch_normalize': '1', 'size': '3'}\n",
      "maxpool_3\n",
      "{'stride': '2', 'size': '2'}\n",
      "convolutional_4\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '32', 'batch_normalize': '1', 'size': '3'}\n",
      "maxpool_5\n",
      "{'stride': '2', 'size': '2'}\n",
      "convolutional_6\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '64', 'batch_normalize': '1', 'size': '3'}\n",
      "maxpool_7\n",
      "{'stride': '2', 'size': '2'}\n",
      "convolutional_8\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'size': '3'}\n",
      "maxpool_9\n",
      "{'stride': '2', 'size': '2'}\n",
      "convolutional_10\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'size': '3'}\n",
      "maxpool_11\n",
      "{'stride': '2', 'size': '2'}\n",
      "convolutional_12\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'size': '3'}\n",
      "maxpool_13\n",
      "{'stride': '1', 'size': '2'}\n",
      "convolutional_14\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '1024', 'batch_normalize': '1', 'size': '3'}\n",
      "convolutional_15\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'size': '1'}\n",
      "convolutional_16\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'size': '3'}\n",
      "convolutional_17\n",
      "{'stride': '1', 'activation': 'linear', 'pad': '1', 'filters': '33', 'size': '1'}\n",
      "yolo_18\n",
      "{'jitter': '.3', 'anchors': '10,14,  23,27,  37,58,  81,82,  135,169,  344,319', 'random': '1', 'mask': '3,4,5', 'num': '6', 'classes': '6', 'ignore_thresh': '.7', 'truth_thresh': '1'}\n",
      "route_19\n",
      "{'layers': '-4'}\n",
      "convolutional_20\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'size': '1'}\n",
      "deconvolutional_21\n",
      "{'stride': '2', 'activation': 'leaky', 'pad': '0', 'filters': '128', 'size': '2'}\n",
      "route_22\n",
      "{'layers': '-1, 8'}\n",
      "convolutional_23\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'size': '3'}\n",
      "convolutional_24\n",
      "{'stride': '1', 'activation': 'linear', 'pad': '1', 'filters': '33', 'size': '1'}\n",
      "yolo_25\n",
      "{'jitter': '.3', 'anchors': '10,14,  23,27,  37,58,  81,82,  135,169,  344,319', 'random': '1', 'mask': '1,2,3', 'num': '6', 'classes': '6', 'ignore_thresh': '.7', 'truth_thresh': '1'}\n"
     ]
    }
   ],
   "source": [
    "convert('yolov3-tiny-ncs-test.cfg', 'yolov3-tiny-ncs-test.prototxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Convert YOLO cfg to Caffe prototxt')\n",
    "    parser.add_argument('cfg', type=str, help='YOLO cfg')\n",
    "    parser.add_argument('prototxt', type=str, help='Caffe prototxt')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    convert(args.cfg, args.prototxt)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
