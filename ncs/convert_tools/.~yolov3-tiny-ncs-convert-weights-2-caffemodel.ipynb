{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, datetime  \n",
    "#sys.path.insert(0, '/data/ssd-caffe/py2_caffe/python') \n",
    "sys.path.insert(0, '/data/ssd-caffe/new-yolov3-caffe/python')\n",
    "import caffe  \n",
    "import numpy as np  \n",
    "from collections import OrderedDict\n",
    "from ConfigParser import ConfigParser\n",
    "\n",
    "#from ConfigParser import ConfigParser\n",
    "class uniqdict(OrderedDict):\n",
    "    _unique = 0\n",
    "    def __setitem__(self, key, val):\n",
    "        if isinstance(val, OrderedDict):\n",
    "            self._unique += 1\n",
    "            key += \"_\"+str(self._unique)\n",
    "        OrderedDict.__setitem__(self, key, val)\n",
    "def load_conv2caffe(buf, start, conv_param):  \n",
    "    weight = conv_param[0].data  \n",
    "    bias = conv_param[1].data  \n",
    "    conv_param[1].data[...] = np.reshape(buf[start:start+bias.size], bias.shape);   start = start + bias.size  \n",
    "    conv_param[0].data[...] = np.reshape(buf[start:start+weight.size], weight.shape); start = start + weight.size  \n",
    "    return start\n",
    "def load_fc2caffe(buf, start, fc_param):  \n",
    "    weight = fc_param[0].data  \n",
    "    bias = fc_param[1].data  \n",
    "    fc_param[1].data[...] = np.reshape(buf[start:start+bias.size], bias.shape);   start = start + bias.size  \n",
    "    fc_param[0].data[...] = np.reshape(buf[start:start+weight.size], weight.shape); start = start + weight.size  \n",
    "    return start\n",
    "def load_conv_bn2caffe(buf, start, conv_param, bn_param, scale_param): \n",
    "    conv_weight = conv_param[0].data  \n",
    "    running_mean = bn_param[0].data  \n",
    "    running_var = bn_param[1].data  \n",
    "    scale_weight = scale_param[0].data  \n",
    "    scale_bias = scale_param[1].data      \n",
    "    scale_param[1].data[...] = np.reshape(buf[start:start+scale_bias.size], scale_bias.shape); start = start + scale_bias.size  \n",
    "    #print scale_bias.size  \n",
    "    #print scale_bias  \n",
    "  \n",
    "    scale_param[0].data[...] = np.reshape(buf[start:start+scale_weight.size], scale_weight.shape); start = start + scale_weight.size  \n",
    "    #print scale_weight.size\n",
    "    #print scale_weight\n",
    "  \n",
    "    bn_param[0].data[...] = np.reshape(buf[start:start+running_mean.size], running_mean.shape); start = start + running_mean.size  \n",
    "    #print running_mean.size\n",
    "    #print running_mean\n",
    "  \n",
    "    bn_param[1].data[...] = np.reshape(buf[start:start+running_var.size], running_var.shape); start = start + running_var.size  \n",
    "    #print running_var.size\n",
    "    #print running_var\n",
    "  \n",
    "    bn_param[2].data[...] = np.array([1.0])  \n",
    "    conv_param[0].data[...] = np.reshape(buf[start:start+conv_weight.size], conv_weight.shape); start = start + conv_weight.size  \n",
    "    #print conv_weight.size\n",
    "    #print conv_weight\n",
    "  \n",
    "    return start\n",
    "def darknet2caffe(cfgfile, weightfile, protofile, caffemodel='gene.caffemodel'):  \n",
    "    #net_info = cfg2prototxt(cfgfile)\n",
    "    #save_prototxt(net_info , protofile, region=False)  \n",
    "    print('benchmark')\n",
    "    net = caffe.Net(protofile, caffe.TEST)\n",
    "    k_v_s = [(k, v) for k, v in net.params.items()]\n",
    "    key_vecnums = [(vecs[0], len(vecs[1])) for vecs in k_v_s]\n",
    "    print([(vecs[0], [vecs[1][i].data.shape for i in range(len(vecs[1]))])for vecs in k_v_s])\n",
    "    print('benchmark')\n",
    "    params = net.params\n",
    "    print('benchmark')\n",
    "  \n",
    "    #blocks = parse_cfg(cfgfile)\n",
    "    parser = ConfigParser(dict_type=uniqdict)\n",
    "    parser.read(cfgfile)\n",
    "    blocks = parser.sections()\n",
    "    print(blocks)\n",
    "    \n",
    "  \n",
    "    #Open the weights file  \n",
    "    fp = open(weightfile, \"rb\")  \n",
    "  \n",
    "    #The first 4 values are header information   \n",
    "    # 1. Major version number  \n",
    "    # 2. Minor Version Number  \n",
    "    # 3. Subversion number   \n",
    "    # 4. IMages seen   \n",
    "    header = np.fromfile(fp, dtype = np.int32, count = 5)\n",
    "    #header = np.fromfile(fp, dtype = np.float32, count = 5)\n",
    "    print(header)\n",
    "    #fp = open(weightfile, 'rb')  \n",
    "    #header = np.fromfile(fp, count=5, dtype=np.int32)  \n",
    "    #header = np.ndarray(shape=(5,),dtype='int32',buffer=fp.read(20))  \n",
    "    #print(header)  \n",
    "    buf = np.fromfile(fp, dtype = np.float32)\n",
    "    print('buf len:{0}'.format(len(buf)))\n",
    "    #print(buf)  \n",
    "    fp.close()  \n",
    "  \n",
    "    layers = []  \n",
    "    layer_id = 1  \n",
    "    start = 0  \n",
    "    for block in blocks:\n",
    "        print(block)\n",
    "        if start >= buf.size:  \n",
    "            break\n",
    "        items = dict(parser.items(block))\n",
    "        print(items)\n",
    "        if block.split('_')[0] == 'net':  \n",
    "            continue\n",
    "        elif ((block.split('_')[0] == 'convolutional') or \n",
    "        (block.split('_')[0] == 'deconvolutional')):\n",
    "            batchnorm_followed = False\n",
    "            relu_followed = False\n",
    "            \n",
    "            if 'batch_normalize' in items and items['batch_normalize']:\n",
    "                batchnorm_followed = True\n",
    "            if 'activation' in items and items['activation'] != 'linear':\n",
    "                relu_followed = True\n",
    "            \n",
    "            if items.has_key('name'):  \n",
    "                conv_layer_name = items['name']  \n",
    "                print('has key name ' + conv_layer_name)\n",
    "                bn_layer_name = '%s-bn' % items['name']  \n",
    "                scale_layer_name = '%s-scale' % items['name']  \n",
    "            else:\n",
    "                if(block.split('_')[0] == 'deconvolutional'):\n",
    "                    conv_layer_name = 'layer%d-upsample' % layer_id  \n",
    "                    print('has no name ' + conv_layer_name)\n",
    "                    #bn_layer_name = 'layer%d-bn' % layer_id  \n",
    "                    #scale_layer_name = 'layer%d-scale' % layer_id \n",
    "                else:\n",
    "                    conv_layer_name = 'layer%d-conv' % layer_id  \n",
    "                    print('has no name ' + conv_layer_name)\n",
    "                    bn_layer_name = 'layer%d-bn' % layer_id  \n",
    "                    scale_layer_name = 'layer%d-scale' % layer_id  \n",
    "  \n",
    "            if batchnorm_followed:\n",
    "                print(\"load_conv_bn2caffe:\")\n",
    "                start = load_conv_bn2caffe(buf, start, params[conv_layer_name], \n",
    "                                           params[bn_layer_name], params[scale_layer_name])\n",
    "            else:\n",
    "                print(\"load_conv2caffe:\")\n",
    "                start = load_conv2caffe(buf, start, params[conv_layer_name])  \n",
    "            if(layer_id == 11):\n",
    "                print('layer_id == 11')\n",
    "                layer_id = layer_id + 2\n",
    "            else:\n",
    "                layer_id = layer_id+1\n",
    "            print('start:{0}'.format(start))\n",
    "        elif block.split('_')[0] == 'connected':  \n",
    "            if items.has_key('name'):  \n",
    "                fc_layer_name = items['name']  \n",
    "            else:  \n",
    "                fc_layer_name = 'layer%d-fc' % layer_id  \n",
    "            start = load_fc2caffe(buf, start, params[fc_layer_name])  \n",
    "            layer_id = layer_id+1  \n",
    "        elif block.split('_')[0] == 'maxpool':  \n",
    "            layer_id = layer_id+1  \n",
    "        elif block.split('_')[0] == 'avgpool':  \n",
    "            layer_id = layer_id+1  \n",
    "        elif block.split('_')[0] == 'region':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'route':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'shortcut':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'softmax':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'cost':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'upsample':  \n",
    "            layer_id = layer_id + 1 \n",
    "        else:  \n",
    "            print('unknow layer type %s ' % block.split('_')[0]) \n",
    "            layer_id = layer_id + 1 \n",
    "    print('save caffemodel to %s' % caffemodel)  \n",
    "    net.save(caffemodel)\n",
    "    \n",
    "cfgfile = '/data/github_repos/yolov3-tiny-fit-ncs/ncs/yolov3-tiny-ncs-without-last-maxpool.cfg'\n",
    "weights = '/data/github_repos/yolov3-tiny-fit-ncs/ncs/yolov3-tiny-ncs-without-last-maxpool-refined-anchors_150300.weights'\n",
    "prototxt = '/data/github_repos/yolov3-tiny-fit-ncs/ncs/yolov3-tiny-ncs-without-last-maxpool.prototxt'\n",
    "#caffemodel = 'Jenerated_nolastpooling.caffemodel'\n",
    "caffemodel = (datetime.datetime.now()).strftime(\"%Y%m%d%H%M%S_\") + 'TinyYoloV3NCS.caffemodel'\n",
    "darknet2caffe(cfgfile, weights, prototxt, caffemodel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
