{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, datetime  \n",
    "#sys.path.insert(0, '/data/ssd-caffe/py2_caffe/python') \n",
    "sys.path.insert(0, '/data/ssd-caffe/new-yolov3-caffe/python')\n",
    "import caffe  \n",
    "import numpy as np  \n",
    "from collections import OrderedDict\n",
    "from ConfigParser import ConfigParser\n",
    "#from ConfigParser import ConfigParser\n",
    "class uniqdict(OrderedDict):\n",
    "    _unique = 0\n",
    "    def __setitem__(self, key, val):\n",
    "        if isinstance(val, OrderedDict):\n",
    "            self._unique += 1\n",
    "            key += \"_\"+str(self._unique)\n",
    "        OrderedDict.__setitem__(self, key, val)\n",
    "def load_conv2caffe(buf, start, conv_param):  \n",
    "    weight = conv_param[0].data  \n",
    "    bias = conv_param[1].data  \n",
    "    conv_param[1].data[...] = np.reshape(buf[start:start+bias.size], bias.shape);   start = start + bias.size  \n",
    "    conv_param[0].data[...] = np.reshape(buf[start:start+weight.size], weight.shape); start = start + weight.size  \n",
    "    return start\n",
    "def load_fc2caffe(buf, start, fc_param):  \n",
    "    weight = fc_param[0].data  \n",
    "    bias = fc_param[1].data  \n",
    "    fc_param[1].data[...] = np.reshape(buf[start:start+bias.size], bias.shape);   start = start + bias.size  \n",
    "    fc_param[0].data[...] = np.reshape(buf[start:start+weight.size], weight.shape); start = start + weight.size  \n",
    "    return start\n",
    "def load_conv_bn2caffe(buf, start, conv_param, bn_param, scale_param): \n",
    "    conv_weight = conv_param[0].data  \n",
    "    running_mean = bn_param[0].data  \n",
    "    running_var = bn_param[1].data  \n",
    "    scale_weight = scale_param[0].data  \n",
    "    scale_bias = scale_param[1].data      \n",
    "    scale_param[1].data[...] = np.reshape(buf[start:start+scale_bias.size], scale_bias.shape); start = start + scale_bias.size  \n",
    "    #print scale_bias.size  \n",
    "    #print scale_bias  \n",
    "  \n",
    "    scale_param[0].data[...] = np.reshape(buf[start:start+scale_weight.size], scale_weight.shape); start = start + scale_weight.size  \n",
    "    #print scale_weight.size\n",
    "    #print scale_weight\n",
    "  \n",
    "    bn_param[0].data[...] = np.reshape(buf[start:start+running_mean.size], running_mean.shape); start = start + running_mean.size  \n",
    "    #print running_mean.size\n",
    "    #print running_mean\n",
    "  \n",
    "    bn_param[1].data[...] = np.reshape(buf[start:start+running_var.size], running_var.shape); start = start + running_var.size  \n",
    "    #print running_var.size\n",
    "    #print running_var\n",
    "  \n",
    "    bn_param[2].data[...] = np.array([1.0])  \n",
    "    conv_param[0].data[...] = np.reshape(buf[start:start+conv_weight.size], conv_weight.shape); start = start + conv_weight.size  \n",
    "    #print conv_weight.size\n",
    "    #print conv_weight\n",
    "  \n",
    "    return start\n",
    "def darknet2caffe(cfgfile, weightfile, protofile, caffemodel='gene.caffemodel'):  \n",
    "    #net_info = cfg2prototxt(cfgfile)\n",
    "    #save_prototxt(net_info , protofile, region=False)  \n",
    "    print('benchmark')\n",
    "    net = caffe.Net(protofile, caffe.TEST)\n",
    "    k_v_s = [(k, v) for k, v in net.params.items()]\n",
    "    key_vecnums = [(vecs[0], len(vecs[1])) for vecs in k_v_s]\n",
    "    print([(vecs[0], [vecs[1][i].data.shape for i in range(len(vecs[1]))])for vecs in k_v_s])\n",
    "    print('benchmark')\n",
    "    params = net.params\n",
    "    print('benchmark')\n",
    "  \n",
    "    #blocks = parse_cfg(cfgfile)\n",
    "    parser = ConfigParser(dict_type=uniqdict)\n",
    "    parser.read(cfgfile)\n",
    "    blocks = parser.sections()\n",
    "    print(blocks)\n",
    "    \n",
    "  \n",
    "    #Open the weights file  \n",
    "    fp = open(weightfile, \"rb\")  \n",
    "  \n",
    "    #The first 4 values are header information   \n",
    "    # 1. Major version number  \n",
    "    # 2. Minor Version Number  \n",
    "    # 3. Subversion number   \n",
    "    # 4. IMages seen   \n",
    "    header = np.fromfile(fp, dtype = np.int32, count = 5)\n",
    "    #header = np.fromfile(fp, dtype = np.float32, count = 5)\n",
    "    print(header)\n",
    "    #fp = open(weightfile, 'rb')  \n",
    "    #header = np.fromfile(fp, count=5, dtype=np.int32)  \n",
    "    #header = np.ndarray(shape=(5,),dtype='int32',buffer=fp.read(20))  \n",
    "    #print(header)  \n",
    "    buf = np.fromfile(fp, dtype = np.float32)\n",
    "    print('buf len:{0}'.format(len(buf)))\n",
    "    #print(buf)  \n",
    "    fp.close()  \n",
    "  \n",
    "    layers = []  \n",
    "    layer_id = 1  \n",
    "    start = 0  \n",
    "    for block in blocks:\n",
    "        print(block)\n",
    "        if start >= buf.size:  \n",
    "            break\n",
    "        items = dict(parser.items(block))\n",
    "        print(items)\n",
    "        if block.split('_')[0] == 'net':  \n",
    "            continue\n",
    "        elif ((block.split('_')[0] == 'convolutional') or \n",
    "        (block.split('_')[0] == 'deconvolutional')):\n",
    "            batchnorm_followed = False\n",
    "            relu_followed = False\n",
    "            \n",
    "            if 'batch_normalize' in items and items['batch_normalize']:\n",
    "                batchnorm_followed = True\n",
    "            if 'activation' in items and items['activation'] != 'linear':\n",
    "                relu_followed = True\n",
    "            \n",
    "            if items.has_key('name'):  \n",
    "                conv_layer_name = items['name']  \n",
    "                print('has key name ' + conv_layer_name)\n",
    "                bn_layer_name = '%s-bn' % items['name']  \n",
    "                scale_layer_name = '%s-scale' % items['name']  \n",
    "            else:\n",
    "                if(block.split('_')[0] == 'deconvolutional'):\n",
    "                    conv_layer_name = 'layer%d-upsample' % layer_id  \n",
    "                    print('has no name ' + conv_layer_name)\n",
    "                    #bn_layer_name = 'layer%d-bn' % layer_id  \n",
    "                    #scale_layer_name = 'layer%d-scale' % layer_id \n",
    "                else:\n",
    "                    conv_layer_name = 'layer%d-conv' % layer_id  \n",
    "                    print('has no name ' + conv_layer_name)\n",
    "                    bn_layer_name = 'layer%d-bn' % layer_id  \n",
    "                    scale_layer_name = 'layer%d-scale' % layer_id  \n",
    "  \n",
    "            if batchnorm_followed:\n",
    "                print(\"load_conv_bn2caffe:\")\n",
    "                start = load_conv_bn2caffe(buf, start, params[conv_layer_name], \n",
    "                                           params[bn_layer_name], params[scale_layer_name])\n",
    "            else:\n",
    "                print(\"load_conv2caffe:\")\n",
    "                start = load_conv2caffe(buf, start, params[conv_layer_name])  \n",
    "            if(layer_id == 11):\n",
    "                print('layer_id == 11')\n",
    "                layer_id = layer_id + 2\n",
    "            else:\n",
    "                layer_id = layer_id+1\n",
    "            print('start:{0}'.format(start))\n",
    "        elif block.split('_')[0] == 'connected':  \n",
    "            if items.has_key('name'):  \n",
    "                fc_layer_name = items['name']  \n",
    "            else:  \n",
    "                fc_layer_name = 'layer%d-fc' % layer_id  \n",
    "            start = load_fc2caffe(buf, start, params[fc_layer_name])  \n",
    "            layer_id = layer_id+1  \n",
    "        elif block.split('_')[0] == 'maxpool':  \n",
    "            layer_id = layer_id+1  \n",
    "        elif block.split('_')[0] == 'avgpool':  \n",
    "            layer_id = layer_id+1  \n",
    "        elif block.split('_')[0] == 'region':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'route':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'shortcut':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'softmax':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'cost':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'upsample':  \n",
    "            layer_id = layer_id + 1 \n",
    "        else:  \n",
    "            print('unknow layer type %s ' % block.split('_')[0]) \n",
    "            layer_id = layer_id + 1 \n",
    "    print('save caffemodel to %s' % caffemodel)  \n",
    "    net.save(caffemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmark\n",
      "[('layer1-conv', [(16, 3, 3, 3)]), ('layer1-bn', [(16,), (16,), (1,)]), ('layer1-scale', [(16,), (16,)]), ('layer3-conv', [(32, 16, 3, 3)]), ('layer3-bn', [(32,), (32,), (1,)]), ('layer3-scale', [(32,), (32,)]), ('layer5-conv', [(64, 32, 3, 3)]), ('layer5-bn', [(64,), (64,), (1,)]), ('layer5-scale', [(64,), (64,)]), ('layer7-conv', [(128, 64, 3, 3)]), ('layer7-bn', [(128,), (128,), (1,)]), ('layer7-scale', [(128,), (128,)]), ('layer9-conv', [(256, 128, 3, 3)]), ('layer9-bn', [(256,), (256,), (1,)]), ('layer9-scale', [(256,), (256,)]), ('layer11-conv', [(512, 256, 3, 3)]), ('layer11-bn', [(512,), (512,), (1,)]), ('layer11-scale', [(512,), (512,)]), ('layer13-conv', [(1024, 512, 3, 3)]), ('layer13-bn', [(1024,), (1024,), (1,)]), ('layer13-scale', [(1024,), (1024,)]), ('layer14-conv', [(256, 1024, 1, 1)]), ('layer14-bn', [(256,), (256,), (1,)]), ('layer14-scale', [(256,), (256,)]), ('layer15-conv', [(512, 256, 3, 3)]), ('layer15-bn', [(512,), (512,), (1,)]), ('layer15-scale', [(512,), (512,)]), ('layer16-conv', [(33, 512, 1, 1), (33,)]), ('layer19-conv', [(128, 256, 1, 1)]), ('layer19-bn', [(128,), (128,), (1,)]), ('layer19-scale', [(128,), (128,)]), ('layer20-upsample', [(128, 128, 2, 2), (128,)]), ('layer22-conv', [(256, 384, 3, 3)]), ('layer22-bn', [(256,), (256,), (1,)]), ('layer22-scale', [(256,), (256,)]), ('layer23-conv', [(33, 256, 1, 1), (33,)])]\n",
      "benchmark\n",
      "benchmark\n",
      "['net_1', 'convolutional_2', 'maxpool_3', 'convolutional_4', 'maxpool_5', 'convolutional_6', 'maxpool_7', 'convolutional_8', 'maxpool_9', 'convolutional_10', 'maxpool_11', 'convolutional_12', 'convolutional_13', 'convolutional_14', 'convolutional_15', 'convolutional_16', 'yolo_17', 'route_18', 'convolutional_19', 'deconvolutional_20', 'route_21', 'convolutional_22', 'convolutional_23', 'yolo_24']\n",
<<<<<<< HEAD
      "[      0       2       0 7180800       0]\n",
=======
      "[      0       2       0 9619200       0]\n",
>>>>>>> cfc2a59d08f8d0545d34c83921b843e1236eb00d
      "buf len:8753458\n",
      "net_1\n",
      "{'hue': '.1', 'saturation': '1.5', 'angle': '0', 'decay': '0.0005', 'learning_rate': '0.001', 'scales': '.1,.1', 'batch': '1', 'height': '416', 'channels': '3', 'width': '416', 'subdivisions': '1', 'burn_in': '1000', 'policy': 'steps', 'max_batches': '500200', 'steps': '400000,450000', 'momentum': '0.9', 'exposure': '1.5'}\n",
      "convolutional_2\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '16', 'batch_normalize': '1', 'size': '3'}\n",
      "has no name layer1-conv\n",
      "load_conv_bn2caffe:\n",
      "start:496\n",
      "maxpool_3\n",
      "{'stride': '2', 'size': '2'}\n",
      "convolutional_4\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '32', 'batch_normalize': '1', 'size': '3'}\n",
      "has no name layer3-conv\n",
      "load_conv_bn2caffe:\n",
      "start:5232\n",
      "maxpool_5\n",
      "{'stride': '2', 'size': '2'}\n",
      "convolutional_6\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '64', 'batch_normalize': '1', 'size': '3'}\n",
      "has no name layer5-conv\n",
      "load_conv_bn2caffe:\n",
      "start:23920\n",
      "maxpool_7\n",
      "{'stride': '2', 'size': '2'}\n",
      "convolutional_8\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'size': '3'}\n",
      "has no name layer7-conv\n",
      "load_conv_bn2caffe:\n",
      "start:98160\n",
      "maxpool_9\n",
      "{'stride': '2', 'size': '2'}\n",
      "convolutional_10\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'size': '3'}\n",
      "has no name layer9-conv\n",
      "load_conv_bn2caffe:\n",
      "start:394096\n",
      "maxpool_11\n",
      "{'stride': '2', 'size': '2'}\n",
      "convolutional_12\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'size': '3'}\n",
      "has no name layer11-conv\n",
      "load_conv_bn2caffe:\n",
      "layer_id == 11\n",
      "start:1575792\n",
      "convolutional_13\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '1024', 'batch_normalize': '1', 'size': '3'}\n",
      "has no name layer13-conv\n",
      "load_conv_bn2caffe:\n",
      "start:6298480\n",
      "convolutional_14\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'size': '1'}\n",
      "has no name layer14-conv\n",
      "load_conv_bn2caffe:\n",
      "start:6561648\n",
      "convolutional_15\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '512', 'batch_normalize': '1', 'size': '3'}\n",
      "has no name layer15-conv\n",
      "load_conv_bn2caffe:\n",
      "start:7743344\n",
      "convolutional_16\n",
      "{'stride': '1', 'activation': 'linear', 'pad': '1', 'filters': '33', 'size': '1'}\n",
      "has no name layer16-conv\n",
      "load_conv2caffe:\n",
      "start:7760273\n",
      "yolo_17\n",
      "{'jitter': '.3', 'anchors': '10,14,  23,27,  37,58,  81,82,  135,169,  344,319', 'random': '1', 'mask': '3,4,5', 'num': '6', 'classes': '6', 'ignore_thresh': '.7', 'truth_thresh': '1'}\n",
      "unknow layer type yolo \n",
      "route_18\n",
      "{'layers': '-4'}\n",
      "convolutional_19\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '128', 'batch_normalize': '1', 'size': '1'}\n",
      "has no name layer19-conv\n",
      "load_conv_bn2caffe:\n",
      "start:7793553\n",
      "deconvolutional_20\n",
      "{'stride': '2', 'activation': 'leaky', 'pad': '0', 'filters': '128', 'size': '2'}\n",
      "has no name layer20-upsample\n",
      "load_conv2caffe:\n",
      "start:7859217\n",
      "route_21\n",
      "{'layers': '-1, 8'}\n",
      "convolutional_22\n",
      "{'activation': 'leaky', 'stride': '1', 'pad': '1', 'filters': '256', 'batch_normalize': '1', 'size': '3'}\n",
      "has no name layer22-conv\n",
      "load_conv_bn2caffe:\n",
      "start:8744977\n",
      "convolutional_23\n",
      "{'stride': '1', 'activation': 'linear', 'pad': '1', 'filters': '33', 'size': '1'}\n",
      "has no name layer23-conv\n",
      "load_conv2caffe:\n",
      "start:8753458\n",
      "yolo_24\n",
<<<<<<< HEAD
      "save caffemodel to 20190409160046_TinyYoloV3NCS.caffemodel\n"
=======
      "save caffemodel to 20190218113600_TinyYoloV3NCS.caffemodel\n"
>>>>>>> cfc2a59d08f8d0545d34c83921b843e1236eb00d
     ]
    }
   ],
   "source": [
    "cfgfile = '/data/github_repos/yolov3-tiny-fit-ncs/ncs/yolov3-tiny-ncs-without-last-maxpool.cfg'\n",
<<<<<<< HEAD
    "weights = '/yolov3-tiny-ncs-without-last-maxpool-refined-anchors_112100.backup'\n",
=======
    "weights = '/data/github_repos/yolov3-tiny-fit-ncs/ncs/yolov3-tiny-ncs-without-last-maxpool-refined-anchors_150300.weights'\n",
>>>>>>> cfc2a59d08f8d0545d34c83921b843e1236eb00d
    "prototxt = '/data/github_repos/yolov3-tiny-fit-ncs/ncs/yolov3-tiny-ncs-without-last-maxpool.prototxt'\n",
    "#caffemodel = 'Jenerated_nolastpooling.caffemodel'\n",
    "caffemodel = (datetime.datetime.now()).strftime(\"%Y%m%d%H%M%S_\") + 'TinyYoloV3NCS.caffemodel'\n",
    "darknet2caffe(cfgfile, weights, prototxt, caffemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "import os, sys, datetime \n",
    "if(len(sys.argv) < 2):\n",
    "    print('please input yolov3-tiny-ncs-weights file')\n",
    "    exit(1)\n",
    "weights = sys.argv[1]\n",
    "\n",
    "#sys.path.insert(0, '/data/ssd-caffe/py2_caffe/python') \n",
    "sys.path.insert(0, '/data/ssd-caffe/new-yolov3-caffe/python')\n",
    "import caffe  \n",
    "import numpy as np  \n",
    "from collections import OrderedDict\n",
    "from ConfigParser import ConfigParser\n",
    "\n",
    "\n",
    "#from ConfigParser import ConfigParser\n",
    "class uniqdict(OrderedDict):\n",
    "    _unique = 0\n",
    "    def __setitem__(self, key, val):\n",
    "        if isinstance(val, OrderedDict):\n",
    "            self._unique += 1\n",
    "            key += \"_\"+str(self._unique)\n",
    "        OrderedDict.__setitem__(self, key, val)\n",
    "def load_conv2caffe(buf, start, conv_param):  \n",
    "    weight = conv_param[0].data  \n",
    "    bias = conv_param[1].data  \n",
    "    conv_param[1].data[...] = np.reshape(buf[start:start+bias.size], bias.shape);   start = start + bias.size  \n",
    "    conv_param[0].data[...] = np.reshape(buf[start:start+weight.size], weight.shape); start = start + weight.size  \n",
    "    return start\n",
    "def load_fc2caffe(buf, start, fc_param):  \n",
    "    weight = fc_param[0].data  \n",
    "    bias = fc_param[1].data  \n",
    "    fc_param[1].data[...] = np.reshape(buf[start:start+bias.size], bias.shape);   start = start + bias.size  \n",
    "    fc_param[0].data[...] = np.reshape(buf[start:start+weight.size], weight.shape); start = start + weight.size  \n",
    "    return start\n",
    "def load_conv_bn2caffe(buf, start, conv_param, bn_param, scale_param): \n",
    "    conv_weight = conv_param[0].data  \n",
    "    running_mean = bn_param[0].data  \n",
    "    running_var = bn_param[1].data  \n",
    "    scale_weight = scale_param[0].data  \n",
    "    scale_bias = scale_param[1].data      \n",
    "    scale_param[1].data[...] = np.reshape(buf[start:start+scale_bias.size], scale_bias.shape); start = start + scale_bias.size  \n",
    "    #print scale_bias.size  \n",
    "    #print scale_bias  \n",
    "  \n",
    "    scale_param[0].data[...] = np.reshape(buf[start:start+scale_weight.size], scale_weight.shape); start = start + scale_weight.size  \n",
    "    #print scale_weight.size\n",
    "    #print scale_weight\n",
    "  \n",
    "    bn_param[0].data[...] = np.reshape(buf[start:start+running_mean.size], running_mean.shape); start = start + running_mean.size  \n",
    "    #print running_mean.size\n",
    "    #print running_mean\n",
    "  \n",
    "    bn_param[1].data[...] = np.reshape(buf[start:start+running_var.size], running_var.shape); start = start + running_var.size  \n",
    "    #print running_var.size\n",
    "    #print running_var\n",
    "  \n",
    "    bn_param[2].data[...] = np.array([1.0])  \n",
    "    conv_param[0].data[...] = np.reshape(buf[start:start+conv_weight.size], conv_weight.shape); start = start + conv_weight.size  \n",
    "    #print conv_weight.size\n",
    "    #print conv_weight\n",
    "  \n",
    "    return start\n",
    "def darknet2caffe(cfgfile, weightfile, protofile, caffemodel='gene.caffemodel'):  \n",
    "    #net_info = cfg2prototxt(cfgfile)\n",
    "    #save_prototxt(net_info , protofile, region=False)  \n",
    "    print('benchmark')\n",
    "    net = caffe.Net(protofile, caffe.TEST)\n",
    "    k_v_s = [(k, v) for k, v in net.params.items()]\n",
    "    key_vecnums = [(vecs[0], len(vecs[1])) for vecs in k_v_s]\n",
    "    print([(vecs[0], [vecs[1][i].data.shape for i in range(len(vecs[1]))])for vecs in k_v_s])\n",
    "    print('benchmark')\n",
    "    params = net.params\n",
    "    print('benchmark')\n",
    "  \n",
    "    #blocks = parse_cfg(cfgfile)\n",
    "    parser = ConfigParser(dict_type=uniqdict)\n",
    "    parser.read(cfgfile)\n",
    "    blocks = parser.sections()\n",
    "    print(blocks)\n",
    "    \n",
    "  \n",
    "    #Open the weights file  \n",
    "    fp = open(weightfile, \"rb\")  \n",
    "  \n",
    "    #The first 4 values are header information   \n",
    "    # 1. Major version number  \n",
    "    # 2. Minor Version Number  \n",
    "    # 3. Subversion number   \n",
    "    # 4. IMages seen   \n",
    "    header = np.fromfile(fp, dtype = np.int32, count = 5)\n",
    "    #header = np.fromfile(fp, dtype = np.float32, count = 5)\n",
    "    print(header)\n",
    "    #fp = open(weightfile, 'rb')  \n",
    "    #header = np.fromfile(fp, count=5, dtype=np.int32)  \n",
    "    #header = np.ndarray(shape=(5,),dtype='int32',buffer=fp.read(20))  \n",
    "    #print(header)  \n",
    "    buf = np.fromfile(fp, dtype = np.float32)\n",
    "    print('buf len:{0}'.format(len(buf)))\n",
    "    #print(buf)  \n",
    "    fp.close()  \n",
    "  \n",
    "    layers = []  \n",
    "    layer_id = 1  \n",
    "    start = 0  \n",
    "    for block in blocks:\n",
    "        print(block)\n",
    "        if start >= buf.size:  \n",
    "            break\n",
    "        items = dict(parser.items(block))\n",
    "        print(items)\n",
    "        if block.split('_')[0] == 'net':  \n",
    "            continue\n",
    "        elif ((block.split('_')[0] == 'convolutional') or \n",
    "        (block.split('_')[0] == 'deconvolutional')):\n",
    "            batchnorm_followed = False\n",
    "            relu_followed = False\n",
    "            \n",
    "            if 'batch_normalize' in items and items['batch_normalize']:\n",
    "                batchnorm_followed = True\n",
    "            if 'activation' in items and items['activation'] != 'linear':\n",
    "                relu_followed = True\n",
    "            \n",
    "            if items.has_key('name'):  \n",
    "                conv_layer_name = items['name']  \n",
    "                print('has key name ' + conv_layer_name)\n",
    "                bn_layer_name = '%s-bn' % items['name']  \n",
    "                scale_layer_name = '%s-scale' % items['name']  \n",
    "            else:\n",
    "                if(block.split('_')[0] == 'deconvolutional'):\n",
    "                    conv_layer_name = 'layer%d-upsample' % layer_id  \n",
    "                    print('has no name ' + conv_layer_name)\n",
    "                    #bn_layer_name = 'layer%d-bn' % layer_id  \n",
    "                    #scale_layer_name = 'layer%d-scale' % layer_id \n",
    "                else:\n",
    "                    conv_layer_name = 'layer%d-conv' % layer_id  \n",
    "                    print('has no name ' + conv_layer_name)\n",
    "                    bn_layer_name = 'layer%d-bn' % layer_id  \n",
    "                    scale_layer_name = 'layer%d-scale' % layer_id  \n",
    "  \n",
    "            if batchnorm_followed:\n",
    "                print(\"load_conv_bn2caffe:\")\n",
    "                start = load_conv_bn2caffe(buf, start, params[conv_layer_name], \n",
    "                                           params[bn_layer_name], params[scale_layer_name])\n",
    "            else:\n",
    "                print(\"load_conv2caffe:\")\n",
    "                start = load_conv2caffe(buf, start, params[conv_layer_name])  \n",
    "            if(layer_id == 11):\n",
    "                print('layer_id == 11')\n",
    "                layer_id = layer_id + 2\n",
    "            else:\n",
    "                layer_id = layer_id+1\n",
    "            print('start:{0}'.format(start))\n",
    "        elif block.split('_')[0] == 'connected':  \n",
    "            if items.has_key('name'):  \n",
    "                fc_layer_name = items['name']  \n",
    "            else:  \n",
    "                fc_layer_name = 'layer%d-fc' % layer_id  \n",
    "            start = load_fc2caffe(buf, start, params[fc_layer_name])  \n",
    "            layer_id = layer_id+1  \n",
    "        elif block.split('_')[0] == 'maxpool':  \n",
    "            layer_id = layer_id+1  \n",
    "        elif block.split('_')[0] == 'avgpool':  \n",
    "            layer_id = layer_id+1  \n",
    "        elif block.split('_')[0] == 'region':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'route':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'shortcut':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'softmax':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'cost':  \n",
    "            layer_id = layer_id + 1  \n",
    "        elif block.split('_')[0] == 'upsample':  \n",
    "            layer_id = layer_id + 1 \n",
    "        else:  \n",
    "            print('unknow layer type %s ' % block.split('_')[0]) \n",
    "            layer_id = layer_id + 1 \n",
    "    print('save caffemodel to %s' % caffemodel)  \n",
    "    net.save(caffemodel)\n",
    "    \n",
    "cfgfile = '/data/github_repos/yolov3-tiny-fit-ncs/ncs/yolov3-tiny-ncs-without-last-maxpool.cfg'\n",
    "#weights = weights\n",
    "prototxt = '/data/github_repos/yolov3-tiny-fit-ncs/ncs/yolov3-tiny-ncs-without-last-maxpool.prototxt'\n",
    "#caffemodel = 'Jenerated_nolastpooling.caffemodel'\n",
    "caffemodel = (datetime.datetime.now()).strftime(\"%Y%m%d%H%M%S_\") + 'TinyYoloV3NCS.caffemodel'\n",
    "darknet2caffe(cfgfile, weights, prototxt, caffemodel)\n",
    "\n",
    "mvnc_command = 'generate-graph.sh ' + caffemodel\n",
    "os.system(mvnc_command)\n",
    "set_ncs2_env_command = 'source /opt/intel/computer_vision_sdk/bin/setupvars.sh'\n",
    "os.system(set_ncs2_env_command)"
=======
    "if __name__ == '__main__':\n",
    "    import sys  \n",
    "    if len(sys.argv) != 5:  \n",
    "        print('try:')  \n",
    "        print('python darknet2caffe.py tiny-yolo-voc.cfg tiny-yolo-voc.weights tiny-yolo-voc.prototxt tiny-yolo-voc.caffemodel')  \n",
    "        print('')  \n",
    "        print('please add name field for each block to avoid generated name')  \n",
    "        exit()  \n",
    "  \n",
    "    cfgfile = sys.argv[1]  \n",
    "    #net_info = cfg2prototxt(cfgfile)  \n",
    "    #print_prototxt(net_info)  \n",
    "    #save_prototxt(net_info, 'tmp.prototxt')  \n",
    "    weightfile = sys.argv[2]  \n",
    "    protofile = sys.argv[3]  \n",
    "    caffemodel = sys.argv[4]  \n",
    "    darknet2caffe(cfgfile, weightfile, protofile, caffemodel)\n"
>>>>>>> cfc2a59d08f8d0545d34c83921b843e1236eb00d
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
