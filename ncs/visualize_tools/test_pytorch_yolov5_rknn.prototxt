name: "Darkent2Caffe"
layer {
  name: "data"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 208
      dim: 208
    }
  }
}
layer {
    bottom: "data"
    top: "layer-conv0"
    name: "layer-conv0"
    type: "Convolution"
    convolution_param {
        num_output: 32
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "layer-conv0"
    top: "layer-conv0"
    name: "conv0-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "layer-conv0"
    top: "layer-conv0"
    name: "conv0-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer-conv0"
    top: "layer-conv0"
    name: "conv0-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
###############focus end#####################################
###############conv1 start#####################################

layer {
    bottom: "layer-conv0"
    top: "layer-conv1"
    name: "layer-conv1"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 3
        pad: 1
        stride: 2
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "layer-conv1"
    top: "layer-conv1"
    name: "conv1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "layer-conv1"
    top: "layer-conv1"
    name: "conv1-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer-conv1"
    top: "layer-conv1"
    name: "conv1-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
############conv1 end#############################################
############btnkcsp1 start#############################################

layer {
    bottom: "layer-conv1"
    top: "btnkcsp-cv1"
    name: "btnkcsp-cv1"
    type: "Convolution"
    convolution_param {
        num_output: 32
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "btnkcsp-cv1"
    top: "btnkcsp-cv1"
    name: "btnkcsp-cv1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnkcsp-cv1"
    top: "btnkcsp-cv1"
    name: "btnkcsp-cv1-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnkcsp-cv1"
    top: "btnkcsp-cv1"
    name: "btnkcsp-cv1-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
############btnkcsp-cv1 end#############################################

#########################btnck1 start###############################
layer {
    bottom: "btnkcsp-cv1"
    top: "btnk-cv1"
    name: "btnk-cv1"
    type: "Convolution"
    convolution_param {
        num_output: 32
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}


layer {
    bottom: "btnk-cv1"
    top: "btnk-cv1"
    name: "btnk-cv1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnk-cv1"
    top: "btnk-cv1"
    name: "btnk-cv1-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnk-cv1"
    top: "btnk-cv1"
    name: "btnk-cv1-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}


layer {
    bottom: "btnk-cv1"
    top: "btnk-cv2"
    name: "btnk-cv2"
    type: "Convolution"
    convolution_param {
        num_output: 32
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "btnk-cv2"
    top: "btnk-cv2"
    name: "btnk-cv2-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnk-cv2"
    top: "btnk-cv2"
    name: "btnk-cv2-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnk-cv2"
    top: "btnk-cv2"
    name: "btnk-cv2-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}

layer {
    bottom: "btnk-cv2"
    bottom: "btnkcsp-cv1"
    top: "btnkc1-out"
    name: "btnkc1-out"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
############btnk1 end#############################################

layer {
    bottom: "btnkc1-out"
    top: "btnkcsp-cv2"
    name: "btnkcsp-cv2"
    type: "Convolution"
    convolution_param {
        num_output: 32
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}

layer {
    bottom: "layer-conv1"
    top: "btnkcsp-cv3"
    name: "btnkcsp-cv3"
    type: "Convolution"
    convolution_param {
        num_output: 32
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}


layer {
    bottom: "btnkcsp-cv2"
    bottom: "btnkcsp-cv3"
    top: "btnkcsp-concat1"
    name: "btnkcsp-concat1"
    type: "Concat"
}

layer {
    bottom: "btnkcsp-concat1"
    top: "btnkcsp-concat1-bn"
    name: "btnkcsp-concat1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnkcsp-concat1-bn"
    top: "btnkcsp-concat1-bn-act"
    name: "btnkcsp-concat1-bn-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}

layer {
    bottom: "btnkcsp-concat1-bn-act"
    top: "btnkcsp1-end-conv"
    name: "btnkcsp1-end-conv"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}

layer {
    bottom: "btnkcsp1-end-conv"
    top: "btnkcsp1-end-conv"
    name: "btnkcsp1-end-conv-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnkcsp1-end-conv"
    top: "btnkcsp1-end-conv"
    name: "btnkcsp1-end-conv-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnkcsp1-end-conv"
    top: "btnkcsp1-end-conv"
    name: "btnkcsp1-end-conv-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}

#########################btnckcsp1 end###############################
##########################module conv down start################################

layer {
    bottom: "btnkcsp1-end-conv"
    top: "conv_d_2"
    name: "conv_d_2"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 3
        pad: 1
        stride: 2
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}

layer {
    bottom: "conv_d_2"
    top: "conv_d_2"
    name: "conv_d_2-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "conv_d_2"
    top: "conv_d_2"
    name: "conv_d_2-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "conv_d_2"
    top: "conv_d_2"
    name: "conv_d_2-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
##########################module conv down end################################

##########################module btnkcsp2 start################################

layer {
    bottom: "conv_d_2"
    top: "btnkcsp2-cv1"
    name: "btnkcsp2-cv1"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "btnkcsp2-cv1"
    top: "btnkcsp2-cv1"
    name: "btnkcsp2-cv1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnkcsp2-cv1"
    top: "btnkcsp2-cv1"
    name: "btnkcsp2-cv1-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnkcsp2-cv1"
    top: "btnkcsp2-cv1"
    name: "btnkcsp2-cv1-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
############btnkcsp2-cv1 end#############################################

#########################btnck2 start###############################
layer {
    bottom: "btnkcsp2-cv1"
    top: "btnk2-cv1"
    name: "btnk2-cv1"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}


layer {
    bottom: "btnk2-cv1"
    top: "btnk2-cv1"
    name: "btnk2-cv1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnk2-cv1"
    top: "btnk2-cv1"
    name: "btnk2-cv1-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnk2-cv1"
    top: "btnk2-cv1"
    name: "btnk2-cv1-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}


layer {
    bottom: "btnk2-cv1"
    top: "btnk2-cv2"
    name: "btnk2-cv2"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "btnk2-cv2"
    top: "btnk2-cv2"
    name: "btnk2-cv2-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnk2-cv2"
    top: "btnk2-cv2"
    name: "btnk2-cv2-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnk2-cv2"
    top: "btnk2-cv2"
    name: "btnk2-cv2-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}

layer {
    bottom: "btnk2-cv2"
    bottom: "btnkcsp2-cv1"
    top: "btnkc2-out"
    name: "btnkc2-out"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
############btnk2 end#############################################

############btnk3 start###########################################
layer {
    bottom: "btnkc2-out"
    top: "btnk3-cv1"
    name: "btnk3-cv1"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}


layer {
    bottom: "btnk3-cv1"
    top: "btnk3-cv1"
    name: "btnk3-cv1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnk3-cv1"
    top: "btnk3-cv1"
    name: "btnk3-cv1-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnk3-cv1"
    top: "btnk3-cv1"
    name: "btnk3-cv1-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}


layer {
    bottom: "btnk3-cv1"
    top: "btnk3-cv2"
    name: "btnk3-cv2"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "btnk3-cv2"
    top: "btnk3-cv2"
    name: "btnk3-cv2-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnk3-cv2"
    top: "btnk3-cv2"
    name: "btnk3-cv2-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnk3-cv2"
    top: "btnk3-cv2"
    name: "btnk3-cv2-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}

layer {
    bottom: "btnk3-cv2"
    bottom: "btnkc2-out"
    top: "btnkc3-out"
    name: "btnkc3-out"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}

############btnk3 end#############################################

############btnk4 start###########################################
layer {
    bottom: "btnkc3-out"
    top: "btnk4-cv1"
    name: "btnk4-cv1"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}


layer {
    bottom: "btnk4-cv1"
    top: "btnk4-cv1"
    name: "btnk4-cv1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnk4-cv1"
    top: "btnk4-cv1"
    name: "btnk4-cv1-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnk4-cv1"
    top: "btnk4-cv1"
    name: "btnk4-cv1-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}


layer {
    bottom: "btnk4-cv1"
    top: "btnk4-cv2"
    name: "btnk4-cv2"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "btnk4-cv2"
    top: "btnk4-cv2"
    name: "btnk4-cv2-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnk4-cv2"
    top: "btnk4-cv2"
    name: "btnk4-cv2-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnk4-cv2"
    top: "btnk4-cv2"
    name: "btnk4-cv2-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}

layer {
    bottom: "btnk4-cv2"
    bottom: "btnkc3-out"
    top: "btnkc4-out"
    name: "btnkc4-out"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}

############btnk4 end#############################################

layer {
    bottom: "btnkc4-out"
    top: "btnkcsp2-cv2"
    name: "btnkcsp2-cv2"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}

layer {
    bottom: "conv_d_2"
    top: "btnkcsp2-cv3"
    name: "btnkcsp2-cv3"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}


layer {
    bottom: "btnkcsp2-cv2"
    bottom: "btnkcsp2-cv3"
    top: "btnkcsp2-concat1"
    name: "btnkcsp2-concat1"
    type: "Concat"
}

layer {
    bottom: "btnkcsp2-concat1"
    top: "btnkcsp2-concat1-bn"
    name: "btnkcsp2-concat1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnkcsp2-concat1-bn"
    top: "btnkcsp2-concat1-bn-act"
    name: "btnkcsp2-concat1-bn-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}

layer {
    bottom: "btnkcsp2-concat1-bn-act"
    top: "btnkcsp2-end-conv"
    name: "btnkcsp2-end-conv"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}

layer {
    bottom: "btnkcsp2-end-conv"
    top: "btnkcsp2-end-conv"
    name: "btnkcsp2-end-conv-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnkcsp2-end-conv"
    top: "btnkcsp2-end-conv"
    name: "btnkcsp2-end-conv-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnkcsp2-end-conv"
    top: "btnkcsp2-end-conv"
    name: "btnkcsp2-end-conv-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}

#########################btnckcsp2 end###############################

#########################conv_d_3 start###############################
layer {
    bottom: "btnkcsp2-end-conv"
    top: "conv_d_3"
    name: "conv_d_3"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 3
        pad: 1
        stride: 2
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "conv_d_3"
    top: "conv_d_3"
    name: "conv_d_3-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "conv_d_3"
    top: "conv_d_3"
    name: "conv_d_3-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "conv_d_3"
    top: "conv_d_3"
    name: "conv_d_3-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
############conv_d_3 end#############################################

##########################module btnkcsp3 start################################

layer {
    bottom: "conv_d_3"
    top: "btnkcsp3-cv1"
    name: "btnkcsp3-cv1"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "btnkcsp3-cv1"
    top: "btnkcsp3-cv1"
    name: "btnkcsp3-cv1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnkcsp3-cv1"
    top: "btnkcsp3-cv1"
    name: "btnkcsp3-cv1-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnkcsp3-cv1"
    top: "btnkcsp3-cv1"
    name: "btnkcsp3-cv1-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
############btnkcsp3-cv1 end#############################################

#########################btnck5 start###############################
layer {
    bottom: "btnkcsp3-cv1"
    top: "btnk5-cv1"
    name: "btnk5-cv1"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}


layer {
    bottom: "btnk5-cv1"
    top: "btnk5-cv1"
    name: "btnk5-cv1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnk5-cv1"
    top: "btnk5-cv1"
    name: "btnk5-cv1-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnk5-cv1"
    top: "btnk5-cv1"
    name: "btnk5-cv1-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}


layer {
    bottom: "btnk5-cv1"
    top: "btnk5-cv2"
    name: "btnk5-cv2"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "btnk5-cv2"
    top: "btnk5-cv2"
    name: "btnk5-cv2-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnk5-cv2"
    top: "btnk5-cv2"
    name: "btnk5-cv2-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnk5-cv2"
    top: "btnk5-cv2"
    name: "btnk5-cv2-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}

layer {
    bottom: "btnk5-cv2"
    bottom: "btnkcsp3-cv1"
    top: "btnk5-out"
    name: "btnk5-out"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
############btnk5 end#############################################

############btnk6 start###########################################
layer {
    bottom: "btnk5-out"
    top: "btnk6-cv1"
    name: "btnk6-cv1"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}


layer {
    bottom: "btnk6-cv1"
    top: "btnk6-cv1"
    name: "btnk6-cv1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnk6-cv1"
    top: "btnk6-cv1"
    name: "btnk6-cv1-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnk6-cv1"
    top: "btnk6-cv1"
    name: "btnk6-cv1-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}


layer {
    bottom: "btnk6-cv1"
    top: "btnk6-cv2"
    name: "btnk6-cv2"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "btnk6-cv2"
    top: "btnk6-cv2"
    name: "btnk6-cv2-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnk6-cv2"
    top: "btnk6-cv2"
    name: "btnk6-cv2-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnk6-cv2"
    top: "btnk6-cv2"
    name: "btnk6-cv2-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}

layer {
    bottom: "btnk6-cv2"
    bottom: "btnk5-out"
    top: "btnk6-out"
    name: "btnk6-out"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}

############btnk6 end#############################################

############btnk7 start###########################################
layer {
    bottom: "btnk6-out"
    top: "btnk7-cv1"
    name: "btnk7-cv1"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}


layer {
    bottom: "btnk7-cv1"
    top: "btnk7-cv1"
    name: "btnk7-cv1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnk7-cv1"
    top: "btnk7-cv1"
    name: "btnk7-cv1-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnk7-cv1"
    top: "btnk7-cv1"
    name: "btnk7-cv1-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}


layer {
    bottom: "btnk7-cv1"
    top: "btnk7-cv2"
    name: "btnk7-cv2"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "btnk7-cv2"
    top: "btnk7-cv2"
    name: "btnk7-cv2-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnk7-cv2"
    top: "btnk7-cv2"
    name: "btnk7-cv2-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnk7-cv2"
    top: "btnk7-cv2"
    name: "btnk7-cv2-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}

layer {
    bottom: "btnk7-cv2"
    bottom: "btnk6-out"
    top: "btnk7-out"
    name: "btnk7-out"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}

############btnk7 end#############################################

layer {
    bottom: "btnk7-out"
    top: "btnkcsp3-cv2"
    name: "btnkcsp3-cv2"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}

layer {
    bottom: "conv_d_3"
    top: "btnkcsp3-cv3"
    name: "btnkcsp3-cv3"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}


layer {
    bottom: "btnkcsp3-cv2"
    bottom: "btnkcsp3-cv3"
    top: "btnkcsp3-concat1"
    name: "btnkcsp3-concat1"
    type: "Concat"
}

layer {
    bottom: "btnkcsp3-concat1"
    top: "btnkcsp3-concat1-bn"
    name: "btnkcsp3-concat1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnkcsp3-concat1-bn"
    top: "btnkcsp3-concat1-bn-act"
    name: "btnkcsp3-concat1-bn-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}

layer {
    bottom: "btnkcsp3-concat1-bn-act"
    top: "btnkcsp3-end-conv"
    name: "btnkcsp3-end-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}

layer {
    bottom: "btnkcsp3-end-conv"
    top: "btnkcsp3-end-conv"
    name: "btnkcsp3-end-conv-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnkcsp3-end-conv"
    top: "btnkcsp3-end-conv"
    name: "btnkcsp3-end-conv-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnkcsp3-end-conv"
    top: "btnkcsp3-end-conv"
    name: "btnkcsp3-end-conv-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}

#########################btnckcsp2 end###############################

###############conv_d_4 start#####################################

layer {
    bottom: "btnkcsp3-end-conv"
    top: "conv_d_4"
    name: "conv_d_4"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 3
        pad: 1
        stride: 2
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "conv_d_4"
    top: "conv_d_4"
    name: "conv_d_4-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "conv_d_4"
    top: "conv_d_4"
    name: "conv_d_4-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "conv_d_4"
    top: "conv_d_4"
    name: "conv_d_4-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
############conv_d_4 end#############################################

######################SPP start######################################
####################spp conv1 start##################################
layer {
    bottom: "conv_d_4"
    top: "spp_conv1"
    name: "spp_conv1"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "spp_conv1"
    top: "spp_conv1"
    name: "spp_conv1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "spp_conv1"
    top: "spp_conv1"
    name: "spp_conv1-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "spp_conv1"
    top: "spp_conv1"
    name: "spp_conv1-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
####################spp conv1  end##################################
layer {
    bottom: "spp_conv1"
    top: "spp_maxpool_1"
    name: "spp_maxpool_1"
    type: "Pooling"
    pooling_param {
        kernel_size: 5
        pad: 2
        stride: 1
        pool: MAX
    }
}

layer {
    bottom: "spp_conv1"
    top: "spp_maxpool_2"
    name: "spp_maxpool_2"
    type: "Pooling"
    pooling_param {
        kernel_size: 9
        pad: 4
        stride: 1
        pool: MAX
    }
}

layer {
    bottom: "spp_conv1"
    top: "spp_maxpool_3"
    name: "spp_maxpool_3"
    type: "Pooling"
    pooling_param {
        kernel_size: 13
        pad: 6
        stride: 1
        pool: MAX
    }
}

layer {
    bottom: "spp_conv1"
    bottom: "spp_maxpool_1"
    bottom: "spp_maxpool_2"
    bottom: "spp_maxpool_3"
    top: "spp_concat"
    name: "spp_concat"
    type: "Concat"
}

####################spp conv2 start##################################
layer {
    bottom: "spp_concat"
    top: "spp_conv2"
    name: "spp_conv2"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "spp_conv2"
    top: "spp_conv2"
    name: "spp_conv2-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "spp_conv2"
    top: "spp_conv2"
    name: "spp_conv2-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "spp_conv2"
    top: "spp_conv2"
    name: "spp_conv2-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
####################spp conv2  end##################################
######################SPP end########################################

############btnkcsp4 without shotcut start#############################################

layer {
    bottom: "spp_conv2"
    top: "btnkcsp4-cv1"
    name: "btnkcsp4-cv1"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "btnkcsp4-cv1"
    top: "btnkcsp4-cv1"
    name: "btnkcsp4-cv1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnkcsp4-cv1"
    top: "btnkcsp4-cv1"
    name: "btnkcsp4-cv1-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnkcsp4-cv1"
    top: "btnkcsp4-cv1"
    name: "btnkcsp4-cv1-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
############btnkcsp4-cv1 end#############################################

#########################btnk8 start###############################
layer {
    bottom: "btnkcsp4-cv1"
    top: "btnk8-cv1"
    name: "btnk8-cv1"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}


layer {
    bottom: "btnk8-cv1"
    top: "btnk8-cv1"
    name: "btnk8-cv1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnk8-cv1"
    top: "btnk8-cv1"
    name: "btnk8-cv1-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnk8-cv1"
    top: "btnk8-cv1"
    name: "btnk8-cv1-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}


layer {
    bottom: "btnk8-cv1"
    top: "btnk8-cv2"
    name: "btnk8-cv2"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "btnk8-cv2"
    top: "btnk8-cv2"
    name: "btnk8-cv2-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnk8-cv2"
    top: "btnk8-cv2"
    name: "btnk8-cv2-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnk8-cv2"
    top: "btnk8-cv2"
    name: "btnk8-cv2-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
############btnk8 end#############################################

layer {
    bottom: "btnk8-cv2"
    top: "btnkcsp4-cv3"
    name: "btnkcsp4-cv3"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
#####################################
layer {
    bottom: "spp_conv2"
    top: "btnkcsp4-cv2"
    name: "btnkcsp4-cv2"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
####################################
layer {
    bottom: "btnkcsp4-cv3"
    bottom: "btnkcsp4-cv2"
    top: "btnkcsp4-concat1"
    name: "btnkcsp4-concat1"
    type: "Concat"
}

layer {
    bottom: "btnkcsp4-concat1"
    top: "btnkcsp4-concat1-bn"
    name: "btnkcsp4-concat1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnkcsp4-concat1-bn"
    top: "btnkcsp4-concat1-bn-act"
    name: "btnkcsp4-concat1-bn-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}

layer {
    bottom: "btnkcsp4-concat1-bn-act"
    top: "btnkcsp4-end-conv"
    name: "btnkcsp4-end-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}

layer {
    bottom: "btnkcsp4-end-conv"
    top: "btnkcsp4-end-conv"
    name: "btnkcsp4-end-conv-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnkcsp4-end-conv"
    top: "btnkcsp4-end-conv"
    name: "btnkcsp4-end-conv-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnkcsp4-end-conv"
    top: "btnkcsp4-end-conv"
    name: "btnkcsp4-end-conv-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}

#########################btnckcsp4 without shotcut end###############################

###############conv5 start#####################################

layer {
    bottom: "btnkcsp4-end-conv"
    top: "conv5"
    name: "conv5"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "conv5"
    top: "conv5"
    name: "conv5-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "conv5"
    top: "conv5"
    name: "conv5-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "conv5"
    top: "conv5"
    name: "conv5-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
############conv5 end#############################################
############upsample start########################################
layer {
    bottom: "conv5"
    top: "upsample1"
    name: "upsample1"
    type: "Upsample"
    upsample_param {
        scale: 2
    }
}
############upsample end########################################
layer {
    bottom: "upsample1"
    bottom: "btnkcsp3-end-conv"
    top: "concat1-out"
    name: "concat1-out"
    type: "Concat"
}
################################################################

############btnkcsp5 without shotcut start#############################################

layer {
    bottom: "concat1-out"
    top: "btnkcsp5-cv1"
    name: "btnkcsp5-cv1"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "btnkcsp5-cv1"
    top: "btnkcsp5-cv1"
    name: "btnkcsp5-cv1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnkcsp5-cv1"
    top: "btnkcsp5-cv1"
    name: "btnkcsp5-cv1-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnkcsp5-cv1"
    top: "btnkcsp5-cv1"
    name: "btnkcsp5-cv1-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
############btnkcsp5-cv1 end#############################################

#########################btnk9 start###############################
layer {
    bottom: "btnkcsp5-cv1"
    top: "btnk9-cv1"
    name: "btnk9-cv1"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}


layer {
    bottom: "btnk9-cv1"
    top: "btnk9-cv1"
    name: "btnk9-cv1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnk9-cv1"
    top: "btnk9-cv1"
    name: "btnk9-cv1-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnk9-cv1"
    top: "btnk9-cv1"
    name: "btnk9-cv1-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}


layer {
    bottom: "btnk9-cv1"
    top: "btnk9-cv2"
    name: "btnk9-cv2"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "btnk9-cv2"
    top: "btnk9-cv2"
    name: "btnk9-cv2-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnk9-cv2"
    top: "btnk9-cv2"
    name: "btnk9-cv2-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnk9-cv2"
    top: "btnk9-cv2"
    name: "btnk9-cv2-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
############btnk9 end#############################################

layer {
    bottom: "btnk9-cv2"
    top: "btnkcsp5-cv3"
    name: "btnkcsp5-cv3"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
#####################################
layer {
    bottom: "concat1-out"
    top: "btnkcsp5-cv2"
    name: "btnkcsp5-cv2"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
####################################
layer {
    bottom: "btnkcsp5-cv3"
    bottom: "btnkcsp5-cv2"
    top: "btnkcsp5-concat1"
    name: "btnkcsp5-concat1"
    type: "Concat"
}

layer {
    bottom: "btnkcsp5-concat1"
    top: "btnkcsp5-concat1-bn"
    name: "btnkcsp5-concat1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnkcsp5-concat1-bn"
    top: "btnkcsp5-concat1-bn-act"
    name: "btnkcsp5-concat1-bn-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}

layer {
    bottom: "btnkcsp5-concat1-bn-act"
    top: "btnkcsp5-end-conv"
    name: "btnkcsp5-end-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}

layer {
    bottom: "btnkcsp5-end-conv"
    top: "btnkcsp5-end-conv"
    name: "btnkcsp5-end-conv-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnkcsp5-end-conv"
    top: "btnkcsp5-end-conv"
    name: "btnkcsp5-end-conv-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnkcsp5-end-conv"
    top: "btnkcsp5-end-conv"
    name: "btnkcsp5-end-conv-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}

#########################btnckcsp5 without shotcut end###############################

###############conv6 start#####################################

layer {
    bottom: "btnkcsp5-end-conv"
    top: "conv6"
    name: "conv6"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "conv6"
    top: "conv6"
    name: "conv6-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "conv6"
    top: "conv6"
    name: "conv6-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "conv6"
    top: "conv6"
    name: "conv6-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
############conv6 end#############################################
############upsample start########################################
layer {
    bottom: "conv6"
    top: "upsample2"
    name: "upsample2"
    type: "Upsample"
    upsample_param {
        scale: 2
    }
}
############upsample end########################################
layer {
    bottom: "upsample2"
    bottom: "btnkcsp2-end-conv"
    top: "concat2-out"
    name: "concat2-out"
    type: "Concat"
}
################################################################

############btnkcsp6 without shotcut start#############################################

layer {
    bottom: "concat2-out"
    top: "btnkcsp6-cv1"
    name: "btnkcsp6-cv1"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "btnkcsp6-cv1"
    top: "btnkcsp6-cv1"
    name: "btnkcsp6-cv1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnkcsp6-cv1"
    top: "btnkcsp6-cv1"
    name: "btnkcsp6-cv1-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnkcsp6-cv1"
    top: "btnkcsp6-cv1"
    name: "btnkcsp6-cv1-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
############btnkcsp6-cv1 end#############################################

#########################btnk10 start###############################
layer {
    bottom: "btnkcsp6-cv1"
    top: "btnk10-cv1"
    name: "btnk10-cv1"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}


layer {
    bottom: "btnk10-cv1"
    top: "btnk10-cv1"
    name: "btnk10-cv1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnk10-cv1"
    top: "btnk10-cv1"
    name: "btnk10-cv1-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnk10-cv1"
    top: "btnk10-cv1"
    name: "btnk10-cv1-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}


layer {
    bottom: "btnk10-cv1"
    top: "btnk10-cv2"
    name: "btnk10-cv2"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "btnk10-cv2"
    top: "btnk10-cv2"
    name: "btnk10-cv2-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnk10-cv2"
    top: "btnk10-cv2"
    name: "btnk10-cv2-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnk10-cv2"
    top: "btnk10-cv2"
    name: "btnk10-cv2-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
############btnk10 end#############################################

layer {
    bottom: "btnk10-cv2"
    top: "btnkcsp6-cv3"
    name: "btnkcsp6-cv3"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
#####################################
layer {
    bottom: "concat2-out"
    top: "btnkcsp6-cv2"
    name: "btnkcsp6-cv2"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
####################################
layer {
    bottom: "btnkcsp6-cv3"
    bottom: "btnkcsp6-cv2"
    top: "btnkcsp6-concat1"
    name: "btnkcsp6-concat1"
    type: "Concat"
}

layer {
    bottom: "btnkcsp6-concat1"
    top: "btnkcsp6-concat1-bn"
    name: "btnkcsp6-concat1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnkcsp6-concat1-bn"
    top: "btnkcsp6-concat1-bn-act"
    name: "btnkcsp6-concat1-bn-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}

layer {
    bottom: "btnkcsp6-concat1-bn-act"
    top: "btnkcsp6-end-conv"
    name: "btnkcsp6-end-conv"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}

layer {
    bottom: "btnkcsp6-end-conv"
    top: "btnkcsp6-end-conv"
    name: "btnkcsp6-end-conv-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnkcsp6-end-conv"
    top: "btnkcsp6-end-conv"
    name: "btnkcsp6-end-conv-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnkcsp6-end-conv"
    top: "btnkcsp6-end-conv"
    name: "btnkcsp6-end-conv-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}

#########################btnckcsp6 without shotcut end###############################

###############conv_d_5 start#####################################

layer {
    bottom: "btnkcsp6-end-conv"
    top: "conv_d_5"
    name: "conv_d_5"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 3
        pad: 1
        stride: 2
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "conv_d_5"
    top: "conv_d_5"
    name: "conv_d_5-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "conv_d_5"
    top: "conv_d_5"
    name: "conv_d_5-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "conv_d_5"
    top: "conv_d_5"
    name: "conv_d_5-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
############conv_d_5 end#############################################
layer {
    bottom: "conv_d_5"
    bottom: "conv6"
    top: "concat3-out"
    name: "concat3-out"
    type: "Concat"
}
#####################################################################



############btnkcsp7 without shotcut start#############################################

layer {
    bottom: "concat3-out"
    top: "btnkcsp7-cv1"
    name: "btnkcsp7-cv1"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "btnkcsp7-cv1"
    top: "btnkcsp7-cv1"
    name: "btnkcsp7-cv1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnkcsp7-cv1"
    top: "btnkcsp7-cv1"
    name: "btnkcsp7-cv1-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnkcsp7-cv1"
    top: "btnkcsp7-cv1"
    name: "btnkcsp7-cv1-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
############btnkcsp7-cv1 end#############################################

#########################btnk11 start###############################
layer {
    bottom: "btnkcsp7-cv1"
    top: "btnk11-cv1"
    name: "btnk11-cv1"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}


layer {
    bottom: "btnk11-cv1"
    top: "btnk11-cv1"
    name: "btnk11-cv1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnk11-cv1"
    top: "btnk11-cv1"
    name: "btnk11-cv1-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnk11-cv1"
    top: "btnk11-cv1"
    name: "btnk11-cv1-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}


layer {
    bottom: "btnk11-cv1"
    top: "btnk11-cv2"
    name: "btnk11-cv2"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "btnk11-cv2"
    top: "btnk11-cv2"
    name: "btnk11-cv2-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnk11-cv2"
    top: "btnk11-cv2"
    name: "btnk11-cv2-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnk11-cv2"
    top: "btnk11-cv2"
    name: "btnk11-cv2-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
############btnk11 end#############################################

layer {
    bottom: "btnk11-cv2"
    top: "btnkcsp7-cv3"
    name: "btnkcsp7-cv3"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
#####################################
layer {
    bottom: "concat3-out"
    top: "btnkcsp7-cv2"
    name: "btnkcsp7-cv2"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
####################################
layer {
    bottom: "btnkcsp7-cv3"
    bottom: "btnkcsp7-cv2"
    top: "btnkcsp7-concat1"
    name: "btnkcsp7-concat1"
    type: "Concat"
}

layer {
    bottom: "btnkcsp7-concat1"
    top: "btnkcsp7-concat1-bn"
    name: "btnkcsp7-concat1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnkcsp7-concat1-bn"
    top: "btnkcsp7-concat1-bn-act"
    name: "btnkcsp7-concat1-bn-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}

layer {
    bottom: "btnkcsp7-concat1-bn-act"
    top: "btnkcsp7-end-conv"
    name: "btnkcsp7-end-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}

layer {
    bottom: "btnkcsp7-end-conv"
    top: "btnkcsp7-end-conv"
    name: "btnkcsp7-end-conv-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnkcsp7-end-conv"
    top: "btnkcsp7-end-conv"
    name: "btnkcsp7-end-conv-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnkcsp7-end-conv"
    top: "btnkcsp7-end-conv"
    name: "btnkcsp7-end-conv-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}

#########################btnkcsp7 without shotcut end###############################

###############conv_d_6 start#####################################

layer {
    bottom: "btnkcsp7-end-conv"
    top: "conv_d_6"
    name: "conv_d_6"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 3
        pad: 1
        stride: 2
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "conv_d_6"
    top: "conv_d_6"
    name: "conv_d_6-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "conv_d_6"
    top: "conv_d_6"
    name: "conv_d_6-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "conv_d_6"
    top: "conv_d_6"
    name: "conv_d_6-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
############conv_d_6 end#############################################
layer {
    bottom: "conv_d_6"
    bottom: "conv5"
    top: "concat4-out"
    name: "concat4-out"
    type: "Concat"
}
#####################################################################

############btnkcsp8 without shotcut start#############################################

layer {
    bottom: "concat4-out"
    top: "btnkcsp8-cv1"
    name: "btnkcsp8-cv1"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "btnkcsp8-cv1"
    top: "btnkcsp8-cv1"
    name: "btnkcsp8-cv1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnkcsp8-cv1"
    top: "btnkcsp8-cv1"
    name: "btnkcsp8-cv1-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnkcsp8-cv1"
    top: "btnkcsp8-cv1"
    name: "btnkcsp8-cv1-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
############btnkcsp8-cv1 end#############################################

#########################btnk12 start###############################
layer {
    bottom: "btnkcsp8-cv1"
    top: "btnk12-cv1"
    name: "btnk12-cv1"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}


layer {
    bottom: "btnk12-cv1"
    top: "btnk12-cv1"
    name: "btnk12-cv1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnk12-cv1"
    top: "btnk12-cv1"
    name: "btnk12-cv1-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnk12-cv1"
    top: "btnk12-cv1"
    name: "btnk12-cv1-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}


layer {
    bottom: "btnk12-cv1"
    top: "btnk12-cv2"
    name: "btnk12-cv2"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
layer {
    bottom: "btnk12-cv2"
    top: "btnk12-cv2"
    name: "btnk12-cv2-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnk12-cv2"
    top: "btnk12-cv2"
    name: "btnk12-cv2-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnk12-cv2"
    top: "btnk12-cv2"
    name: "btnk12-cv2-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
############btnk12 end#############################################

layer {
    bottom: "btnk12-cv2"
    top: "btnkcsp8-cv3"
    name: "btnkcsp8-cv3"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
#####################################
layer {
    bottom: "concat4-out"
    top: "btnkcsp8-cv2"
    name: "btnkcsp8-cv2"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}
####################################
layer {
    bottom: "btnkcsp8-cv3"
    bottom: "btnkcsp8-cv2"
    top: "btnkcsp8-concat1"
    name: "btnkcsp8-concat1"
    type: "Concat"
}

layer {
    bottom: "btnkcsp8-concat1"
    top: "btnkcsp8-concat1-bn"
    name: "btnkcsp8-concat1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnkcsp8-concat1-bn"
    top: "btnkcsp8-concat1-bn-act"
    name: "btnkcsp8-concat1-bn-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}

layer {
    bottom: "btnkcsp8-concat1-bn-act"
    top: "btnkcsp8-end-conv"
    name: "btnkcsp8-end-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
        weight_filler
        {
            type: "constant" 
            value: 1
        }
    }
}

layer {
    bottom: "btnkcsp8-end-conv"
    top: "btnkcsp8-end-conv"
    name: "btnkcsp8-end-conv-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    bottom: "btnkcsp8-end-conv"
    top: "btnkcsp8-end-conv"
    name: "btnkcsp8-end-conv-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "btnkcsp8-end-conv"
    top: "btnkcsp8-end-conv"
    name: "btnkcsp8-end-conv-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}

#########################btnkcsp8 without shotcut end###############################
layer {
    bottom: "btnkcsp6-end-conv"
    top: "conv_out_1"
    name: "conv_out_1"
    type: "Convolution"
    convolution_param {
        num_output: 36
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: true
        weight_filler
        {
            type: "constant" 
            value: 1
        }
        
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    bottom: "btnkcsp7-end-conv"
    top: "conv_out_2"
    name: "conv_out_2"
    type: "Convolution"
    convolution_param {
        num_output: 36
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: true
        weight_filler
        {
            type: "constant" 
            value: 1
        }
        
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    bottom: "btnkcsp8-end-conv"
    top: "conv_out_3"
    name: "conv_out_3"
    type: "Convolution"
    convolution_param {
        num_output: 36
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: true
        weight_filler
        {
            type: "constant" 
            value: 1
        }
        
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}

